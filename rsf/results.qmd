# Results

```{r}
#| label: setup-results

# Load packages and data
library(dplyr)
library(purrr)
library(tibble)
library(tidyr)
library(ggplot2)
library(forcats)
library(knitr)
library(kableExtra)
library(RankAggreg)
library(splendid)
library(magrittr)
library(DT)
library(plotly)
library(patchwork)
library(tidymodels)
library(themis)
library(RColorBrewer)
library(ggh4x)
library(ggtext)
library(randomForest)
library(here)
library(conflicted)
conflict_prefer("margin", "ggplot2")
conflict_prefer("set_names", "purrr")
conflict_prefer("filter", "dplyr")
conflict_prefer("HKnorm", "nanostringr")

source(here("validation/cs_process_cohorts.R"))
source(here("src/funs.R"))

## Inputs
# Training data
train_data <- readRDS(here("data", "train_data.rds"))
train_class <- readRDS(here("data", "train_class.rds"))

# Confirmation data
conf_data <- readRDS(here("data", "conf_data.rds"))
conf_class <- readRDS(here("data", "conf_class.rds"))

# Validation data
val_data <- readRDS(here("data", "val_data.rds"))
val_class <- readRDS(here("data", "val_class.rds"))

# Sequential data
seq_data <- readRDS(here("data", "seq_data.rds"))
seq_class <- readRDS(here("data", "seq_class.rds"))

# Two-step data
two_step_data <- readRDS(here("data", "two_step_data.rds"))
two_step_class <- readRDS(here("data", "two_step_class.rds"))

## Outputs
# Metrics
all_metrics_train <- readRDS(here("data/all_metrics_train.rds"))
per_class_metrics_seq <-
  readRDS(here("data/per_class_metrics_seq.rds"))
per_class_metrics_two_step <-
  readRDS(here("data/per_class_metrics_two_step.rds"))

# Models
wflow_smote_rf_model <- readRDS(here("data/wflow_smote_rf_model_train.rds"))
all_models_seq <- readRDS(here("data/all_models_seq.rds"))
all_models_two_step <- readRDS(here("data/all_models_two_step.rds"))

# Overall Variable Importance
all_vi_train <- readRDS(here("data/all_vi_train.rds"))
all_vi_seq <- readRDS(here("data/all_vi_seq.rds"))
all_vi_two_step <- readRDS(here("data/all_vi_two_step.rds"))

# Gene Optimization
gene_opt_all_metrics_conf <- 
  readRDS(here("data/gene_opt_all_metrics_conf.rds"))
gene_opt_all_metrics_conf_seq <-
  readRDS(here("data/gene_opt_all_metrics_conf_seq.rds"))
gene_opt_all_metrics_conf_two_step <-
  readRDS(here("data/gene_opt_all_metrics_conf_two_step.rds"))

# PrOTYPE and SPOT genes
coefmat <- readRDS(here("data/coefmat.rds"))
overlap_PrOTYPE <- intersect(names(train_data), cs5$Name)
overlap_SPOT <- intersect(names(train_data), coefmat[["Symbol"]])
overlap_all <- unique(c(overlap_PrOTYPE, overlap_SPOT))

# PrOTYPE final model
final_model <- readRDS(here("data/final_model.rds"))

# Histotypes colour palette
class_palette <- brewer.pal(n = 5, name = "Dark2")
```

We summarize cross-validated training performance of class metrics in the training set. The accuracy, F1-score, and kappa, are the metrics of interest. Workflows are ordered by their mean estimates across the outer folds of the nested CV for each metric.

## Training Set

```{r}
#| label: train-metrics

all_metrics_train <- all_metrics_train %>%
  separate(wflow, c("Subsampling", "Algorithms"), remove = FALSE) %>%
  mutate(
    Subsampling = factor(Subsampling, levels = c("none", "down", "up", "smote", "hybrid")),
    Algorithms = factor(Algorithms, levels = c("rf", "svm", "xgb", "mr")),
    class_group = factor(
      class_group,
      levels = c("Overall", "HGSC", "CCOC", "ENOC", "LGSC", "MUC")
    )
  ) %>% 
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(wflow, class_group, .metric)
  ) %>% 
  mutate(
    undefined = case_when(any(is.nan(mean_estimate)) ~ "all",
                          any(is.na(.estimate)) ~ "some",
                          .default = "none"),
    .by = c(wflow, .metric)
  )
```

::: {#training-set-results .panel-tabset .nav-pills}
### Accuracy

```{r}
#| label: tbl-train-accuracy
#| tbl-cap: 'Training Set Mean Accuracy'
#| tbl-pos: 'H'

all_metrics_train |>
  summarize_metrics(metric = "accuracy") |>
  kbl(booktabs = TRUE, escape = FALSE) |>
  kable_styling() |>
  add_header_above(c(" " = 3, "Histotypes" = 5)) |>
  column_spec(column = 3, border_right = TRUE) |>
  collapse_rows(columns = 1)
```

```{r}
#| label: fig-train-accuracy
#| fig-cap: 'Training Set Mean Accuracy'
#| fig-height: 8
#| fig-width: 7

p <- ggplot(
  all_metrics_train %>% filter(.metric == "accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  scale_colour_viridis_d(begin = 0,
                         end = 0.9,
                         option = "plasma") +
  facet_wrap2(~ class_group,
              ncol = 1,
              strip = strip_themed(background_x = elem_list_rect(fill = c("grey90", class_palette)))) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Workflow",
       y = "Accuracy",
       title = "Training Set Mean Accuracy")
print(p)
```

### Sensitivity

```{r}
#| label: tbl-train-sens
#| tbl-cap: 'Training Set Mean Sensitivity'
#| tbl-pos: 'H'

all_metrics_train |>
  summarize_metrics(metric = "sensitivity") |>
  kbl(booktabs = TRUE, escape = FALSE) |>
  kable_styling() |>
  add_header_above(c(" " = 3, "Histotypes" = 5)) |>
  column_spec(column = 3, border_right = TRUE) |>
  collapse_rows(columns = 1)
```

```{r}
#| label: fig-train-sens
#| fig-cap: 'Training Set Mean Sensitivity'
#| fig-height: 8
#| fig-width: 7

p <- ggplot(
  all_metrics_train %>% filter(.metric == "sensitivity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  scale_colour_viridis_d(begin = 0,
                         end = 0.9,
                         option = "plasma") +
  facet_wrap2(~ class_group,
              ncol = 1,
              strip = strip_themed(background_x = elem_list_rect(fill = c("grey90", class_palette)))) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Workflow",
       y = "Sensitivity",
       title = "Training Set Mean Sensitivity")
print(p)
```

### Specificity

```{r}
#| label: tbl-train-spec
#| tbl-cap: 'Training Set Mean Specificity'
#| tbl-pos: 'H'

all_metrics_train |>
  summarize_metrics(metric = "specificity") |>
  kbl(booktabs = TRUE, escape = FALSE) |>
  kable_styling() |>
  add_header_above(c(" " = 3, "Histotypes" = 5)) |>
  column_spec(column = 3, border_right = TRUE) |>
  collapse_rows(columns = 1)
```

```{r}
#| label: fig-train-spec
#| fig-cap: 'Training Set Mean Specificity'
#| fig-height: 8
#| fig-width: 7

p <- ggplot(
  all_metrics_train %>% filter(.metric == "specificity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  scale_colour_viridis_d(begin = 0,
                         end = 0.9,
                         option = "plasma") +
  facet_wrap2(~ class_group,
              ncol = 1,
              strip = strip_themed(background_x = elem_list_rect(fill = c("grey90", class_palette)))) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Workflow",
       y = "Specificity",
       title = "Training Set Mean Specificity")
print(p)
```

### F1-Score

```{r}
#| label: tbl-train-f1
#| tbl-cap: 'Training Set Mean F1-Score'
#| tbl-pos: 'H'

all_metrics_train |>
  summarize_metrics(metric = "f_meas") |>
  kbl(booktabs = TRUE, escape = FALSE) |>
  kable_styling() |>
  add_header_above(c(" " = 3, "Histotypes" = 5)) |>
  column_spec(column = 3, border_right = TRUE) |>
  collapse_rows(columns = 1)
```

```{r}
#| label: fig-train-f1
#| fig-cap: 'Training Set Mean F1-Score'
#| fig-height: 8
#| fig-width: 7

p <- ggplot(
  all_metrics_train %>% filter(.metric == "f_meas"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  scale_colour_viridis_d(begin = 0,
                         end = 0.9,
                         option = "plasma") +
  facet_wrap2(~ class_group,
              ncol = 1,
              strip = strip_themed(background_x = elem_list_rect(fill = c("grey90", class_palette)))) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Workflow",
       y = "F1-Score",
       title = "Training Set Mean F1-Score")
print(p)
```

### Balanced Accuracy

```{r}
#| label: tbl-train-bal-accuracy
#| tbl-cap: 'Training Set Mean Balanced Accuracy'
#| tbl-pos: 'H'

all_metrics_train |>
  summarize_metrics(metric = "bal_accuracy") |>
  kbl(booktabs = TRUE, escape = FALSE) |>
  kable_styling() |>
  add_header_above(c(" " = 3, "Histotypes" = 5)) |>
  column_spec(column = 3, border_right = TRUE) |>
  collapse_rows(columns = 1)
```

```{r}
#| label: fig-train-bal-accuracy
#| fig-cap: 'Training Set Mean Balanced Accuracy'
#| fig-height: 8
#| fig-width: 7

p <- ggplot(
  all_metrics_train %>% filter(.metric == "bal_accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  scale_colour_viridis_d(begin = 0,
                         end = 0.9,
                         option = "plasma") +
  facet_wrap2(~ class_group,
              ncol = 1,
              strip = strip_themed(background_x = elem_list_rect(fill = c("grey90", class_palette)))) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Workflow",
       y = "Balanced Accuracy",
       title = "Training Set Mean Balanced Accuracy")
print(p)
```

### Kappa

```{r}
#| label: tbl-train-kappa
#| tbl-cap: 'Training Set Mean Kappa'
#| tbl-pos: 'H'

all_metrics_train |>
  summarize_metrics(metric = "kap") |>
  kbl(booktabs = TRUE, escape = FALSE) |>
  kable_styling() |>
  add_header_above(c(" " = 3, "Histotypes" = 5)) |>
  column_spec(column = 3, border_right = TRUE) |>
  collapse_rows(columns = 1)
```

```{r}
#| label: fig-train-kappa
#| fig-cap: 'Training Set Mean Kappa'
#| fig-height: 8
#| fig-width: 7

p <- ggplot(
  all_metrics_train %>% filter(.metric == "kap"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  scale_colour_viridis_d(begin = 0,
                         end = 0.9,
                         option = "plasma") +
  facet_wrap2(~ class_group,
              ncol = 1,
              strip = strip_themed(background_x = elem_list_rect(fill = c("grey90", class_palette)))) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Mean Kappa")
print(p)
```
:::

## Rank Aggregation

Multi-step methods:

-   **sequential**: sequential algorithm sequence of subsampling methods and algorithms used are:
    -   HGSC vs. non-HGSC using upsubsampling and random forest
    -   CCOC vs. non-CCOC using SMOTE subsampling and XGBoost
    -   ENOC vs. non-ENOC using hybrid subsampling and support vector machine
    -   LGSC vs. MUC using hybrid subsampling and random forest
-   **two_step**: two-step algorithm sequence of subsampling methods and algorithms used are:
    -   HGSC vs. non-HGSC using SMOTE subsampling and random forest
    -   CCOC vs. ENOC vs. MUC vs. LGSC using hybrid subsampling and support vector machine

We conduct rank aggregation using a two-stage nested appraoch:

1.  First we rank aggregate the per-class metrics for F1-score, balanced accuracy and kappa.
2.  Then we take the aggregated lists from the three metrics and perform a final rank aggregation.
3.  The top workflows from the final rank aggregation are used for gene optimization in the confirmation set

### Across Classes

```{r}
#| label: per-class-metrics

# Add lower, upper, undefined columns for per-class sequential, two_step
per_class_metrics_seq <- per_class_metrics_seq |>
  mutate(class_group = factor(class_group, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))) |>
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(class_group, .metric)
  ) |>
  mutate(undefined = case_when(any(is.nan(mean_estimate)) ~ "all", any(is.na(.estimate)) ~ "some", .default = "none"),
         .by = .metric) |>
  add_column(wflow = "sequential", .before = 1)

per_class_metrics_two_step <- per_class_metrics_two_step |>
  mutate(class_group = factor(class_group, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))) |>
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(class_group, .metric)
  ) |>
  mutate(undefined = case_when(any(is.nan(mean_estimate)) ~ "all", any(is.na(.estimate)) ~ "some", .default = "none"),
         .by = .metric) |>
  add_column(wflow = "two_step", .before = 1)

# Combine all per-class metrics
per_class_metrics <- bind_rows(all_metrics_train,
                               per_class_metrics_seq,
                               per_class_metrics_two_step) |>
  filter(class_group != "Overall") |>
  droplevels()

# Keep only workflows that predicted at least one case in each class
per_class_metrics_comp <- per_class_metrics %>%
  distinct(pick(-fold_id, -.estimate)) %>%
  select(wflow, .metric, class_group, mean_estimate) %>%
  arrange(class_group) %>%
  pivot_wider(names_from = "class_group", values_from = "mean_estimate") %>%
  drop_na() %>%
  pivot_longer(cols = where(is.numeric),
               names_to = "class_group",
               values_to = "mean_estimate")
```

::: {#rank-agg-metrics .panel-tabset .nav-pills}
#### F1-Score

```{r}
#| label: tbl-rank-agg-f1
#| tbl-cap: 'F1-Score Rank Aggregation Summary'

# Select ranking metric
per_class_f1 <- filter(per_class_metrics_comp, .metric == "f_meas")

# Rank metric
wflow_ranks_f1 <- per_class_f1 %>%
  mutate(rank = factor(row_number(-mean_estimate)), .by = class_group) %>%
  arrange(rank) %>%
  pivot_wider(id_cols = class_group,
              names_from = "rank",
              values_from = "wflow") %>%
  column_to_rownames("class_group") %>%
  as.matrix()

# Rank aggregation of metrics using genetic algorithm
wflow_rank_agg_f1 <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks_f1,
  k = ncol(wflow_ranks_f1),
  method = "GA",
  seed = 2025,
  verbose = FALSE,
  maxIter = 1e4
)) %>% 
  pluck("top.list") %>% 
  enframe(name = "Rank", value = "Workflow")

# Metric summary with rank aggregation
f1_summary <- per_class_f1 |>
  rename(Workflow = wflow) |>
  mutate(mean_estimate = round(mean_estimate, digits = 3)) |>
  inner_join(wflow_rank_agg_f1, by = "Workflow") |>
  pivot_wider(id_cols = c(Workflow, Rank),
              names_from = class_group,
              values_from = mean_estimate) |> 
  arrange(Rank)

# Interactive table showing metrics used in rank aggregation
datatable(
  f1_summary,
  options = list(
    scrollX = TRUE,
    fixedColumns = TRUE,
    columnDefs = list(
      list(
        targets = 1,
        render = JS("$.fn.dataTable.render.ellipsis( 10 )")
      ),
      list(type = "natural", targets = 0)
    ),
    pageLength = 50
  ), 
  rownames = FALSE,
  filter = "top",
  autoHideNavigation = TRUE,
  extensions = "FixedColumns",
  plugins = c("ellipsis", "natural")
)
```

#### Balanced Accuracy

```{r}
#| label: tbl-rank-agg-bal-acc
#| tbl-cap: 'Balanced Accuracy Rank Aggregation Summary'

# Select ranking metric
per_class_bal_acc <- filter(per_class_metrics_comp, .metric == "bal_accuracy")

# Rank metric
wflow_ranks_bal_acc <- per_class_bal_acc %>%
  mutate(rank = factor(row_number(-mean_estimate)), .by = class_group) %>%
  arrange(rank) %>%
  pivot_wider(id_cols = class_group,
              names_from = "rank",
              values_from = "wflow") %>%
  column_to_rownames("class_group") %>%
  as.matrix()

# Rank aggregation of metrics using genetic algorithm
wflow_rank_agg_bal_acc <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks_bal_acc,
  k = ncol(wflow_ranks_bal_acc),
  method = "GA",
  seed = 2025,
  verbose = FALSE,
  maxIter = 1e5
)) %>% 
  pluck("top.list") %>% 
  enframe(name = "Rank", value = "Workflow")

# Metric summary with rank aggregation
bal_acc_summary <- per_class_bal_acc |>
  rename(Workflow = wflow) |>
  mutate(mean_estimate = round(mean_estimate, digits = 3)) |>
  inner_join(wflow_rank_agg_bal_acc, by = "Workflow") |>
  pivot_wider(id_cols = c(Workflow, Rank),
              names_from = class_group,
              values_from = mean_estimate) |> 
  arrange(Rank)

# Interactive table showing metrics used in rank aggregation
datatable(
  bal_acc_summary,
  options = list(
    scrollX = TRUE,
    fixedColumns = TRUE,
    columnDefs = list(
      list(
        targets = 1,
        render = JS("$.fn.dataTable.render.ellipsis( 10 )")
      ),
      list(type = "natural", targets = 0)
    ),
    pageLength = 50
  ), 
  rownames = FALSE,
  filter = "top",
  autoHideNavigation = TRUE,
  extensions = "FixedColumns",
  plugins = c("ellipsis", "natural")
)
```

#### Kappa

```{r}
#| label: tbl-rank-agg-kap
#| tbl-cap: 'Kappa Rank Aggregation Summary'

# Select ranking metric
per_class_kap <- filter(per_class_metrics_comp, .metric == "kap")

# Rank metric
wflow_ranks_kap <- per_class_kap %>%
  mutate(rank = factor(row_number(-mean_estimate)), .by = class_group) %>%
  arrange(rank) %>%
  pivot_wider(id_cols = class_group,
              names_from = "rank",
              values_from = "wflow") %>%
  column_to_rownames("class_group") %>%
  as.matrix()

# Rank aggregation of metrics using genetic algorithm
wflow_rank_agg_kap <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks_kap,
  k = ncol(wflow_ranks_kap),
  method = "GA",
  seed = 2025,
  verbose = FALSE,
  maxIter = 1e4
)) %>% 
  pluck("top.list") %>% 
  enframe(name = "Rank", value = "Workflow")

# Metric summary with rank aggregation
kap_summary <- per_class_kap |>
  rename(Workflow = wflow) |>
  mutate(mean_estimate = round(mean_estimate, digits = 3)) |>
  inner_join(wflow_rank_agg_kap, by = "Workflow") |>
  pivot_wider(id_cols = c(Workflow, Rank),
              names_from = class_group,
              values_from = mean_estimate) |> 
  arrange(Rank)

# Interactive table showing metrics used in rank aggregation
datatable(
  kap_summary,
  options = list(
    scrollX = TRUE,
    fixedColumns = TRUE,
    columnDefs = list(
      list(
        targets = 1,
        render = JS("$.fn.dataTable.render.ellipsis( 10 )")
      ),
      list(type = "natural", targets = 0)
    ),
    pageLength = 50
  ), 
  rownames = FALSE,
  filter = "top",
  autoHideNavigation = TRUE,
  extensions = "FixedColumns",
  plugins = c("ellipsis", "natural")
)
```
:::

### Across Metrics

```{r}
#| label: tbl-rank-agg-comp-metrics
#| tbl-cap: 'Rank Aggregation Comparison of Metrics Used'
#| tbl-pos: 'H'

rank_agg_metrics <- list(F1 = wflow_rank_agg_f1,
                         `Balanced Accuracy` = wflow_rank_agg_bal_acc,
                         Kappa = wflow_rank_agg_kap) |>
  bind_rows(.id = "Metric") |>
  pivot_wider(names_from = "Metric", values_from = "Workflow")

rank_agg_metrics |> 
  kbl(booktabs = TRUE, linesep = "") |> 
  kable_styling(full_width = FALSE)
```

```{r}
#| label: tbl-rank-agg-final
#| tbl-cap: 'Top 5 Workflows from Final Rank Aggregation'
#| tbl-pos: 'H'

# Rank aggregation of aggregated metrics using GA
wflow_ranks_metrics <- rank_agg_metrics |> 
  drop_na() |> 
  pivot_longer(cols = where(is.character), names_to = "Metric", values_to = "Workflow") |> 
  pivot_wider(names_from = Rank, values_from = Workflow) |> 
  column_to_rownames("Metric") |> 
  as.matrix()

# Take top 5 workflows
wflow_rank_agg_metrics <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks_metrics,
  k = 5,
  method = "GA",
  seed = 2025,
  verbose = FALSE
)) |>
  pluck("top.list") %>%
  enframe(name = "Rank", value = "Workflow")

# Number of top workflows
n_wflow_top <- nrow(wflow_rank_agg_metrics)

wflow_rank_agg_metrics |> 
  kbl(booktabs = TRUE, linesep = "") |> 
  kable_styling(full_width = FALSE)
```

### Top Workflows

We look at the per-class evaluation metrics of the top `r n_wflow_top` workflows.

```{r}
#| label: tbl-metrics-top
#| tbl-cap: "Top Workflow Per-Class Evaluation Metrics"

metrics_top <- per_class_metrics |> 
  distinct(pick(-fold_id, -.estimate)) |> 
  rename(Workflow = wflow) |> 
  inner_join(wflow_rank_agg_metrics, by = "Workflow") |> 
  mutate(
    Workflow = fct_reorder(Workflow, Rank),
    .metric = case_match(
      .metric,
      "accuracy" ~ "Accuracy",
      "sensitivity" ~ "Sensitivity",
      "specificity" ~ "Specificity",
      "f_meas" ~ "F1-Score",
      "bal_accuracy" ~ "Balanced Accuracy",
      "kap" ~ "Kappa",
      .ptype = factor(
        levels = c(
          "Accuracy",
          "Sensitivity",
          "Specificity",
          "F1-Score",
          "Balanced Accuracy",
          "Kappa"
        )
      )
    )
  ) |> 
  drop_na(.metric) |> 
  arrange(Rank)

metrics_top_df <- metrics_top |>
  select(Metric = .metric, Workflow, mean_estimate, lower, upper, class_group) |>
  mutate(across(where(is.numeric), ~ round(., digits = 3))) |>
  mutate(Estimate = paste0(mean_estimate, " (", lower, ", ", upper, ")"),
         .keep = "unused") |> 
  arrange(Metric, class_group) |> 
  pivot_wider(names_from = "class_group", values_from = "Estimate")

metrics_top_df |> 
  kbl(booktabs = TRUE, linesep = "") |> 
  kable_styling(full_width = FALSE) |> 
  collapse_rows(columns = 1) |> 
  add_header_above(c(" " = 2, "Histotypes" = 5))
```

```{r}
#| label: fig-top-wflows-by-all-metric
#| fig-cap: !expr paste('Top', n_wflow_top, 'Workflow Per-Class Evaluation Metrics by Metric')
#| fig-height: 7
#| out-width: '100%'

p <-
  ggplot(metrics_top) +
  geom_pointrange(
    aes(
      x = Workflow,
      y = mean_estimate,
      ymin = lower,
      ymax = upper,
      color = class_group
    ),
    position = position_dodge(width = 0.25),
    fatten = 1
  ) +
  scale_color_brewer(palette = "Dark2") + 
  facet_wrap(vars(.metric), ncol = 1, scales = "free_y") +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  ) +
  labs(
    x = "Workflow",
    y = "Metric Value",
    color = "Class",
    title = paste("Top", n_wflow_top, "Workflow Per-Class Evaluation Metrics by Metric")
  )
print(p)
```

```{r}
#| label: tbl-metrics-3-top-ranks
#| tbl-cap: "Top Workflow Per-Class Evaluation Metrics and Ranks"

metrics_3_top_ranks <- bind_rows(
  `F1-Score` = f1_summary,
  `Balanced Accuracy` = bal_acc_summary,
  Kappa = kap_summary,
  .id = "Metric"
) |> 
  semi_join(wflow_rank_agg_metrics, by = "Workflow")

metrics_3_top_ranks |> 
  select(-Metric) |> 
  kbl() |> 
  kable_styling() |> 
  pack_rows(
    group_label = "F1-Score",
    start_row = 1,
    end_row = 5,
    label_row_css = "border-bottom: 1px solid; text-align: center;",
    background = "#fce4d6"
  ) |>
  pack_rows(
    group_label = "Balanced Accuracy",
    start_row = 6,
    end_row = 10,
    label_row_css = "border-bottom: 1px solid; text-align: center;",
    background = "#ededed"
  ) |>
  pack_rows(
    group_label = "Kappa",
    start_row = 11,
    end_row = 15,
    label_row_css = "border-bottom: 1px solid; text-align: center;",
    background = "#e2efda"
  )
```

```{r}
#| label: fig-top-wflows-by-three-metrics
#| fig-cap: !expr paste('Top', n_wflow_top, 'Workflow Per-Class Evaluation Metrics by Metric')
#| fig-width: 10
#| fig-height: 6
#| out-width: '100%'

metrics_3_top <- metrics_top |> 
  filter(.metric %in% c("F1-Score", "Balanced Accuracy", "Kappa")) |> 
  mutate(.metric = droplevels(.metric))

p <-
  ggplot(metrics_3_top) +
  geom_pointrange(
    aes(
      x = class_group,
      y = mean_estimate,
      ymin = lower,
      ymax = upper,
      color = Workflow
    ),
    position = position_dodge(width = 0.5),
    fatten = 1
  ) +
  geom_hline(yintercept = 0.8, color = "grey20", alpha = 0.5, linetype = "dashed") +
  scale_color_brewer(palette = "Set1") +
  scale_y_continuous(breaks = seq(0, 1, 0.2)) +
  facet_wrap(vars(.metric), nrow = 1) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    legend.position = "inside",
    legend.position.inside = c(0.01, 0.01),
    legend.justification.inside = c(0, 0)
  ) +
  labs(
    x = "Workflow",
    y = "Metric Value",
    title = paste("Top", n_wflow_top, "Workflow Per-Class Evaluation Metrics by Metric")
  )
print(p)
```

Misclassified cases from a previous step of the sequence of classifiers are not included in subsequent steps of the training set CV folds. Thus, we cannot piece together the test set predictions from the sequential and two-step algorithms to obtain overall metrics.

## Optimal Gene Sets

### Sequential Algorithm

```{r}
#| label: fig-gene-seq
#| fig-cap: 'Gene Optimization for Sequential Classifier'
#| fig-height: 8
#| fig-width: 5
#| out-width: '100%'
 
gene_seq_names <- all_vi_seq %>% 
  rename(Name = Variable) %>% 
  rowid_to_column("Genes") %>% 
  add_row(Genes = 0L, Name = "Base", .before = 1)

dat_gene_seq <- gene_opt_all_metrics_conf_seq %>% 
  filter(.metric == "f_meas", class_group == "Overall") %>% 
  mutate(
    min_estimate = min(.estimate),
    max_estimate = max(.estimate),
    .by = c(Sequence, Genes)
  ) %>%
  mutate(
    label = paste0(
      "Sequence: ",
      Sequence,
      ", Workflow: ",
      wflow,
      ", Measure: ",
      .metric)
  ) %>% 
  inner_join(gene_seq_names, by = "Genes")

p_gene_seq <-
  ggplot(dat_gene_seq, aes(text = Name)) +
  geom_pointrange(aes(
    x = Genes,
    y = mean_estimate,
    ymin = min_estimate,
    ymax = max_estimate
  )) +
  facet_wrap(~ label, ncol = 1) +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for Sequential Classifier")
ggplotly(p_gene_seq, width = 600, height = 600)

n_genes_seq <- 9
genes_add_seq <- gene_seq_names %>%
  filter(Genes > 0L) %>% 
  slice_min(order_by = Genes, n = n_genes_seq) %>% 
  pull(Name)
opt_genes_seq <- c(overlap_all, genes_add_seq)
```

In the sequential algorithm, all sequences have relatively flat average F1-scores across the number of genes added. However, we can observe in sequence 4, the F1-score is highest when we reach `r n_genes_seq` genes added, hence the optimal number of genes used will be n=`r length(overlap_all)`+`r n_genes_seq`=`r length(opt_genes_seq)` The added genes are: `r str_flatten_comma(genes_add_seq, last = " and ")`.

```{r}
#| label: tbl-gene-opt-seq
#| tbl-cap: 'Gene Profile of Optimal Set in Sequential Algorithm'
#| tbl-pos: 'H'

candidates_all <- gene_seq_names |>
  filter(Genes > 0) |>
  pull(Name)
unused_genes_seq <- gene_seq_names |>
  filter(Genes > 0) |>
  slice_max(order_by = Genes, n = -n_genes_seq) |>
  arrange(Genes) |>
  pull(Name)
gene_order_seq <- c(opt_genes_seq, unused_genes_seq)

gene_dist <-
  data.frame(Genes = gene_order_seq) |>
  mutate(Set = ifelse(Genes %in% overlap_all, "Base", "Candidates"),
         .before = Genes) |> 
  mutate(
    PrOTYPE = ifelse(Genes %in% overlap_PrOTYPE, cli::symbol$tick, ""),
    SPOT = ifelse(Genes %in% overlap_SPOT, cli::symbol$tick, ""),
    `Optimal Set` = case_when(
      Genes %in% opt_genes_seq ~ cli::symbol$circle_filled,
      Set == "Candidates" & !Genes %in% opt_genes_seq ~ cli::symbol$circle_filled,
      .default = ""
    ),
    `Optimal Set` = cell_spec(`Optimal Set`,
                              color = case_when(Genes %in% overlap_all ~ "green",
                                                Genes %in% opt_genes_seq ~ "#8B8000",
                                                .default = "#FF0000")),
    `Candidate Rank` = ifelse(Genes %in% overlap_all, "",
                              match(Genes, candidates_all))
  )

gene_dist |> 
  kbl(booktabs = TRUE,
      longtable = TRUE,
      linesep = "",
      escape = FALSE) |> 
  kable_styling(full_width = FALSE) |> 
  collapse_rows(columns = 1)
```

### SMOTE-Random Forest

```{r}
#| label: fig-gene-smote-rf
#| fig-cap: 'Gene Optimization for SMOTE-Random Forest Classifier'
#| fig-height: 5
#| fig-width: 5
#| out-width: '100%'

gene_smote_rf_names <- all_vi_train %>% 
  filter(wflow == "smote_rf") %>% 
  rowid_to_column("Genes") %>% 
  select(Genes, Name = Variable) %>% 
  add_row(Genes = 0L, Name = "Base", .before = 1)

dat_gene_smote_rf <- gene_opt_all_metrics_conf %>% 
  filter(.metric == "f_meas", class_group == "Overall") %>% 
  mutate(
    min_estimate = min(.estimate),
    max_estimate = max(.estimate),
    .by = Genes
  ) %>%
  mutate(label = paste0("Workflow: smote_rf, Measure: ", .metric)) %>% 
  inner_join(gene_smote_rf_names, by = "Genes")

p_gene_smote_rf <- 
  ggplot(dat_gene_smote_rf, aes(text = Name)) +
  geom_pointrange(aes(
    x = Genes,
    y = mean_estimate,
    ymin = min_estimate,
    ymax = max_estimate
  )) +
  facet_wrap(~ label, ncol = 1) +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for SMOTE-Random Forest Classifier")
ggplotly(p_gene_smote_rf, width = 600, height = 400)

n_genes_smote_rf <- 16
genes_add_smote_rf <- gene_smote_rf_names %>% 
  filter(Genes > 0L) %>% 
  slice_min(order_by = Genes, n = n_genes_smote_rf) %>% 
  pull(Name)
opt_genes_smote_rf <- c(overlap_all, genes_add_smote_rf)
```

In the SMOTE-Random Forest classifier, the mean F1-score is highest when we reach `r n_genes_smote_rf` genes added, hence the optimal number of genes used will be n=`r length(overlap_all)`+`r n_genes_smote_rf`=`r length(opt_genes_smote_rf)` The added genes are: `r str_flatten_comma(genes_add_smote_rf, last = " and ")`.

```{r}
#| label: tbl-gene-opt-smote-rf
#| tbl-cap: 'Gene Profile of Optimal Set in SMOTE-Random Forest Workflow'
#| tbl-pos: 'H'

candidates_all <- gene_smote_rf_names |>
  filter(Genes > 0) |>
  pull(Name)
unused_genes_smote_rf <- gene_smote_rf_names |>
  filter(Genes > 0) |>
  slice_max(order_by = Genes, n = -n_genes_smote_rf) |> 
  arrange(Genes) |> 
  pull(Name)
gene_order_smote_rf <- c(opt_genes_smote_rf, unused_genes_smote_rf)

gene_dist <-
  data.frame(Genes = gene_order_smote_rf) |>
  mutate(Set = ifelse(Genes %in% overlap_all, "Base", "Candidates"),
         .before = Genes) |> 
  mutate(
    PrOTYPE = ifelse(Genes %in% overlap_PrOTYPE, cli::symbol$tick, ""),
    SPOT = ifelse(Genes %in% overlap_SPOT, cli::symbol$tick, ""),
    `Optimal Set` = case_when(
      Genes %in% opt_genes_smote_rf ~ cli::symbol$circle_filled,
      Set == "Candidates" & !Genes %in% opt_genes_smote_rf ~ cli::symbol$circle_filled,
      .default = ""
    ),
    `Optimal Set` = cell_spec(`Optimal Set`,
                              color = case_when(Genes %in% overlap_all ~ "green",
                                                Genes %in% opt_genes_smote_rf ~ "#8B8000",
                                                .default = "#FF0000")),
    `Candidate Rank` = ifelse(Genes %in% overlap_all, "",
                              match(Genes, candidates_all))
  )

gene_dist |> 
  kbl(booktabs = TRUE,
      longtable = TRUE,
      linesep = "",
      escape = FALSE) |> 
  kable_styling(full_width = FALSE) |> 
  collapse_rows(columns = 1)
```

### Two-Step

```{r}
#| label: fig-gene-two-step
#| fig-cap: 'Gene Optimization for Two-Step Classifier'
#| fig-height: 6
#| fig-width: 5
#| out-width: '100%'

gene_two_step_names <- all_vi_two_step %>% 
  rename(Name = Variable) %>% 
  rowid_to_column("Genes") %>% 
  add_row(Genes = 0L, Name = "Base", .before = 1)

dat_gene_two_step <- gene_opt_all_metrics_conf_two_step %>% 
  filter(.metric == "f_meas", class_group == "Overall") %>% 
  mutate(
    min_estimate = min(.estimate),
    max_estimate = max(.estimate),
    .by = c(Sequence, Genes)
  ) %>%
  mutate(
    label = paste0(
      "Sequence: ",
      Sequence,
      ", Workflow: ",
      wflow,
      ", Measure: ",
      .metric)
  ) %>% 
  inner_join(gene_two_step_names, by = "Genes")

p_gene_two_step <-
  ggplot(dat_gene_two_step, aes(text = Name)) +
  geom_pointrange(aes(
    x = Genes,
    y = mean_estimate,
    ymin = min_estimate,
    ymax = max_estimate
  )) +
  facet_wrap(~ label, ncol = 1) +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for Two-Step Classifier")
ggplotly(p_gene_two_step, width = 600, height = 600)

n_genes_two_step <- 15
genes_add_two_step <- gene_two_step_names %>%
  filter(Genes > 0L) %>% 
  slice_min(order_by = Genes, n = n_genes_two_step) %>% 
  pull(Name)
opt_genes_two_step <- c(overlap_all, genes_add_two_step)
```

```{r}
#| label: tbl-gene-opt-two-step
#| tbl-cap: 'Gene Profile of Optimal Set in Two-Step Workflow'
#| tbl-pos: 'H'

candidates_all <- gene_two_step_names |>
  filter(Genes > 0) |>
  pull(Name)
unused_genes_two_step <- gene_two_step_names |>
  filter(Genes > 0) |> 
  slice_max(order_by = Genes, n = -n_genes_two_step) |> 
  arrange(Genes) |> 
  pull(Name)
gene_order_two_step <- c(opt_genes_two_step, unused_genes_two_step)

gene_dist <-
  data.frame(Genes = gene_order_two_step) |>
  mutate(Set = ifelse(Genes %in% overlap_all, "Base", "Candidates"),
         .before = Genes) |> 
  mutate(
    PrOTYPE = ifelse(Genes %in% overlap_PrOTYPE, cli::symbol$tick, ""),
    SPOT = ifelse(Genes %in% overlap_SPOT, cli::symbol$tick, ""),
    `Optimal Set` = case_when(
      Genes %in% opt_genes_two_step ~ cli::symbol$circle_filled,
      Set == "Candidates" & !Genes %in% opt_genes_two_step ~ cli::symbol$circle_filled,
      .default = ""
    ),
    `Optimal Set` = cell_spec(`Optimal Set`,
                              color = case_when(Genes %in% overlap_all ~ "green",
                                                Genes %in% opt_genes_two_step ~ "#8B8000",
                                                .default = "#FF0000")),
    `Candidate Rank` = ifelse(Genes %in% overlap_all, "",
                              match(Genes, candidates_all))
  )

gene_dist |> 
  kbl(booktabs = TRUE,
      longtable = TRUE,
      linesep = "",
      escape = FALSE) |> 
  kable_styling(full_width = FALSE) |> 
  collapse_rows(columns = 1)
```

## Test Set Performance

Now we'd like to see how our best methods perform in the confirmation and validation sets. The class-specific F1-scores will be used.

The top 2 methods are the sequential and SMOTE-Random Forest classifiers. We can test 2 additional methods by using either the full set of genes or the optimal set of genes for both of these classifiers.

```{r}
#| label: top-models

# Sequential full model
mod_seq_full <- list(all_models_seq,
                     seq_data,
                     seq_class) |> 
  pmap(~ {
    set.seed(2024)
    ..1 |> fit(data = cbind(..2, class = ..3))
  })

# Sequential optimal gene list model
mod_seq_opt <- list(all_models_seq,
                     seq_data,
                     seq_class) |> 
  pmap(~ {
    set.seed(2024)
    wflow <- ..1 |>
      pluck("pre", "actions", "recipe", "recipe") |>
      modify_at(c("var_info", "term_info"), \(x) x |>
                  arrange(match(variable, gene_order_seq)) |>
                  filter(variable %in% c(opt_genes_seq, "class"))) |> 
      update_recipe(x = ..1, recipe = _)
    set.seed(2024)
    wflow |>
      fit(data = cbind(..2, class = ..3))
  })

# Training data
train_ref <- train_data |>
  mutate(class = factor(train_class, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))
                                                
# smote_rf full model trained
set.seed(2024)
mod_smote_rf_full <- wflow_smote_rf_model |>
  fit(data = train_ref)

# smote_rf optimal gene list workflow
set.seed(2024)
wflow_smote_rf_opt <- wflow_smote_rf_model |>
  pluck("pre", "actions", "recipe", "recipe") |>
  modify_at(c("var_info", "term_info"),
            \(x) x |>
              filter(variable %in% c(opt_genes_smote_rf, "class")) |>
              arrange(match(variable, gene_order_smote_rf))) |>
  update_recipe(x = wflow_smote_rf_model, recipe = _)

# smote_rf optimal gene list model trained
set.seed(2024)
mod_smote_rf_opt <- wflow_smote_rf_opt |>
  fit(data = train_ref)

# Two-step full model
mod_two_step_full <- list(all_models_two_step,
                          two_step_data,
                          two_step_class) |> 
  pmap(~ {
    set.seed(2024)
    ..1 |> fit(data = cbind(..2, class = ..3))
  })

# Two-step optimal gene list model
mod_two_step_opt <- list(all_models_two_step,
                         two_step_data,
                         two_step_class) |> 
  pmap(~ {
    set.seed(2024)
    wflow <- ..1 |>
      pluck("pre", "actions", "recipe", "recipe") |>
      modify_at(c("var_info", "term_info"), \(x) x |>
                  arrange(match(variable, gene_order_two_step)) |> 
                  filter(variable %in% c(opt_genes_two_step, "class"))) |> 
      update_recipe(x = ..1, recipe = _)
    set.seed(2024)
    wflow |>
      fit(data = cbind(..2, class = ..3))
  })

# Per-class metric set
per_class_mset <-
  metric_set(accuracy, sensitivity, specificity, f_meas, bal_accuracy, kap)
```

### Confirmation Set

```{r}
#| label: tbl-conf-eval
#| tbl-cap: 'Evaluation Metrics on Confirmation Set Models'
#| tbl-pos: 'H'

# Confirmation set overall predictions
conf_pred_overall <- data.frame(FileName = rownames(conf_data),
                                Truth = conf_class) %>%
  mutate(
    c(p_seq_full = mod_seq_full,
      p_seq_opt = mod_seq_opt,
      p_smote_rf_full = list(mod_smote_rf_full),
      p_smote_rf_opt = list(mod_smote_rf_opt),
      p_two_step_full = mod_two_step_full,
      p_two_step_opt = mod_two_step_opt
    ) %>%
      map(~ predict(., conf_data)[[".pred_class"]]) %>%
      bind_cols()
  ) %>%
  rename_with(~ gsub("(.*)\\..*_(.*)", "\\1_\\2", .), where(is.factor)) %>% 
  mutate(
    across(matches("p"), as.character),
    Prediction_sequential_full = case_when(
      !grepl("non", p_seq_full_s1) ~ p_seq_full_s1,
      !grepl("non", p_seq_full_s2) ~ p_seq_full_s2,
      !grepl("non", p_seq_full_s3) ~ p_seq_full_s3,
      .default = p_seq_full_s4
    ),
    Prediction_sequential_optimal = case_when(
      !grepl("non", p_seq_opt_s1) ~ p_seq_opt_s1,
      !grepl("non", p_seq_opt_s2) ~ p_seq_opt_s2,
      !grepl("non", p_seq_opt_s3) ~ p_seq_opt_s3,
      .default = p_seq_opt_s4
    ),
    Prediction_smote_rf_full = p_smote_rf_full,
    Prediction_smote_rf_opt = p_smote_rf_opt,
    Prediction_two_step_full = ifelse(!grepl("non", p_two_step_full_s1),
                                      p_two_step_full_s1, p_two_step_full_s2),
    Prediction_two_step_optimal = ifelse(!grepl("non", p_two_step_opt_s1),
                                         p_two_step_opt_s1, p_two_step_opt_s2), 
    across(c("Truth", matches("Prediction")), factor)
  ) |> 
  select(-starts_with("p", ignore.case = FALSE)) %>%
  pivot_longer(
    cols = matches("Prediction"),
    names_to = "Method",
    names_prefix = "Prediction_",
    values_to = "Prediction"
  ) %>% 
  mutate(across(c(Truth, Prediction),
                ~ factor(.x, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))) |> 
  mutate(Method = factor(
    case_match(
      Method,
      "sequential_full" ~ "Sequential, Full Set",
      "sequential_optimal" ~ "Sequential, Optimal Set",
      "smote_rf_full" ~ "SMOTE-Random Forest, Full Set",
      "smote_rf_opt" ~ "SMOTE-Random Forest, Optimal Set",
      "two_step_full" ~ "Two-Step, Full Set",
      "two_step_optimal" ~ "Two-Step, Optimal Set"
    )
  ))

# Confirmation set overall metrics
conf_eval_overall <- conf_pred_overall %>%
  group_by(Method) %>%
  summarize(
    Accuracy = accuracy_vec(Truth, Prediction),
    Sensitivity = sensitivity_vec(Truth, Prediction),
    Specificity = specificity_vec(Truth, Prediction),
    `F1-Score` = f_meas_vec(Truth, Prediction),
    `Balanced Accuracy` = bal_accuracy_vec(Truth, Prediction),
    Kappa = kap_vec(Truth, Prediction)
  ) |> 
  pivot_longer(
    cols = where(is.numeric),
    names_to = "Metric",
    values_to = "Overall"
  )

# Confirmation set per-class metrics
conf_eval_per_class <- conf_pred_overall %>%
  nest(.by = Method) %>%
  mutate(data = map(data, ~ ova_metrics(.x, Truth, Prediction, per_class_mset))) %>%
  unnest(data) %>%
  mutate(
    Metric = case_match(
      .metric,
      "accuracy" ~ "Accuracy",
      "sensitivity" ~ "Sensitivity",
      "specificity" ~ "Specificity",
      "f_meas" ~ "F1-Score",
      "bal_accuracy" ~ "Balanced Accuracy",
      "kap" ~ "Kappa"
    ),
    .keep = "unused"
  ) %>%
  pivot_wider(
    id_cols = c(Method, Metric),
    names_from = class_group,
    values_from = .estimate
  )

# Combined all metrics
conf_eval <-
  inner_join(conf_eval_overall,
             conf_eval_per_class,
             by = c("Method", "Metric"))
  
conf_eval |> 
  kbl(booktabs = TRUE, linesep = "", digits = 3) |> 
  kable_styling(latex_options = "scale_down") |> 
  collapse_rows(columns = 1) |>  
  add_header_above(c(" " = 3, "Histotypes" = 5))
```

```{r}
#| label: fig-conf-entropy
#| fig-cap: 'Entropy vs. Predicted Probability in Confirmation Set'
#| fig-width: 10
#| fig-height: 6
#| out-width: '100%'

conf_entropy <- data.frame(FileName = rownames(conf_data),
                           Truth = conf_class) %>%
  mutate(
    c(p_seq_full = mod_seq_full,
      p_seq_opt = mod_seq_opt,
      p_smote_rf_full = list(smote_rf_full = mod_smote_rf_full),
      p_smote_rf_opt = list(smote_rf_opt = mod_smote_rf_opt),
      p_two_step_full = mod_two_step_full,
      p_two_step_opt = mod_two_step_opt
    ) |>
      map(~ {
        predict(., conf_data, type = "prob") |>
          rowwise() |>
          mutate(
            prob = max(c_across(where(is.numeric))),
            entropy = entropy::entropy(c_across(matches(".pred")), unit = "log2")
          ) |>
          ungroup()
      }) |>
      list_cbind() |>
      unnest_wider(col = everything(), names_sep =  "/")
  ) |>
  pivot_longer(
    cols = where(is.numeric),
    names_to = c("Method", "Workflow", "Metric"),
    names_pattern = "(.*)\\.(.*)/(.*)",
    values_to = "Value"
  ) |>
  mutate(Method = factor(
    case_match(
      Method,
      "p_seq_full" ~ "Sequential, Full Set",
      "p_seq_opt" ~ "Sequential, Optimal Set",
      "p_smote_rf_full" ~ "SMOTE-Random Forest, Full Set",
      "p_smote_rf_opt" ~ "SMOTE-Random Forest, Optimal Set",
      "p_two_step_full" ~ "Two-Step, Full Set",
      "p_two_step_opt" ~ "Two-Step, Optimal Set"
    )
  )) |>
  inner_join(conf_pred_overall, by = join_by(FileName, Truth, Method)) |>
  filter(Metric %in% c("prob", "entropy")) |>
  filter(
    (grepl("Sequential", Method) & (
      Prediction == "HGSC" & grepl("s1", Workflow) |
        Prediction == "CCOC" & grepl("s2", Workflow) |
        Prediction == "ENOC" & grepl("s3", Workflow) |
        Prediction %in% c("MUC", "LGSC") & grepl("s4", Workflow)
    )) |
    grepl("SMOTE-Random Forest", Method) |
    (grepl("Two-Step", Method) & (
      Prediction == "HGSC" & grepl("s1", Workflow) |
        Prediction != "HGSC" & grepl("s2", Workflow)
    ))
  ) |>
  pivot_wider(names_from = Metric, values_from = Value)

p <- ggplot(conf_entropy) +
  geom_point(aes(x = prob, y = entropy, color = Prediction), alpha = 0.5) +
  scale_color_brewer(palette = "Dark2") +
  theme_bw() +
  theme(panel.grid.minor = element_blank()) +
  facet_wrap(vars(Method), ncol = 2) +
  labs(
    x = "Probability",
    y = "Entropy (log base 2)",
    title = "Entropy vs. Predicted Probability in Confirmation Set"
  )
print(p)
```

```{r}
#| label: fig-conf-metrics-gene-opt
#| fig-cap: 'Gene Optimized Workflows Per-Class Metrics in Confirmation Set'
#| fig-width: 10
#| fig-height: 6
#| out-width: '100%'

conf_metrics_top <- conf_eval_per_class |>
  pivot_longer(
    cols = where(is.numeric),
    names_to = "Class",
    names_ptypes = list(Class = factor(
      levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")
    )),
    values_to = "Value"
  ) |>
  filter(Metric %in% c("F1-Score", "Balanced Accuracy", "Kappa")) |> 
  mutate(Metric = fct_inorder(Metric))

p <- ggplot(conf_metrics_top) +
  geom_col(
    aes(x = Class, y = Value, fill = Method),
    position = position_dodge()
  ) +
  geom_hline(yintercept = 0.8, color = "grey20", alpha = 0.5, linetype = "dashed") +
  scale_fill_brewer(palette = "Paired") +
  scale_y_continuous(breaks = seq(0, 1, 0.2)) +
  facet_wrap(vars(Metric), nrow = 1) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    legend.position = "bottom"
  ) +
  labs(
    y = "Metric Value",
    title = paste("Gene-Optimized Workflows Per-Class Metrics in Confirmation Set")
  )
print(p)
```

```{r}
#| label: fig-conf-conf-mat
#| fig-cap: 'Confusion Matrices for Confirmation Set Models'
#| fig-height: 9
#| fig-width: 8
#| out-width: '100%'

conf_conf_mat <- conf_pred_overall %>%
  nest(.by = Method) %>%
  deframe() %>%
  imap(
    ~ conf_mat(.x, Truth, Prediction) %>%
      autoplot(type = "heatmap") +
      scale_fill_distiller(palette = "YlOrBr", direction = 1) +
      labs(title = .y) +
      theme(
        axis.ticks = element_blank(),
        axis.text.y = element_markdown(colour = rev(class_palette), margin = margin(r = -5)),
        axis.text.x = element_markdown(colour = class_palette, margin = margin(t = -3))
      )
  ) %>%
  wrap_plots(nrow = 3) +
  plot_annotation(title = "Confusion Matrices for Confirmation Set Models", tag_levels = "A")

print(conf_conf_mat)
```

::: {#conf-roc-curves .panel-tabset .nav-pills}
```{r}
#| label: roc-curve-setup

conf_data_seq <- list(conf_data,
                      conf_data[conf_class != "HGSC",],
                      conf_data[!conf_class %in% c("HGSC", "CCOC"),],
                      conf_data[!conf_class %in% c("HGSC", "CCOC", "ENOC"),])

conf_class_seq <- list(
  fct_other(conf_class, keep = "HGSC", other_level = "non-HGSC"),
  fct_other(conf_class[conf_class != "HGSC"],  keep = "CCOC", other_level = "non-CCOC"),
  fct_other(conf_class[!conf_class %in% c("HGSC", "CCOC")],  keep = "ENOC", other_level = "non-ENOC"),
  factor(conf_class[!conf_class %in% c("HGSC", "CCOC", "ENOC")])
)

conf_data_two_step <- list(conf_data,
                           conf_data[conf_class != "HGSC", ])

conf_class_two_step <-
  list(factor(ifelse(conf_class == "HGSC", "HGSC", "non-HGSC")),
       factor(conf_class[conf_class != "HGSC"]))

class_groups_seq <-
  set_names(class_palette, paste0("(?<!non_)", c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))) |>
  imap_chr(~ paste0(
    "<span style = 'color:",
    .x,
    ";'>",
    gsub("(?<!non_)", "", .y, fixed = TRUE),
    "</span>"
  )) |>
  str_replace_all(string = map_chr(map(conf_class_seq, levels), ~ gsub("-", "_", paste(., collapse = " vs. "))), pattern = _)

class_groups_two_step <- 
    set_names(class_palette, paste0("(?<!non_)", c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))) |>
  imap_chr(~ paste0(
    "<span style = 'color:",
    .x,
    ";'>",
    gsub("(?<!non_)", "", .y, fixed = TRUE),
    "</span>"
  )) |>
  str_replace_all(string = map_chr(map(conf_class_two_step, levels), ~ gsub("-", "_", paste(., collapse = " vs. "))), pattern = _)
```

#### Sequential, Full

```{r}
#| label: fig-roc-curve-seq-full
#| fig-cap: 'ROC Curves for Sequential Full Model in Confirmation Set'
#| fig-height: 7
#| fig-width: 7
#| out-width: '100%'

roc_curve_seq_full <- mod_seq_full %>% 
  set_names(class_groups_seq) |> 
  list(conf_data_seq, conf_class_seq) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")%>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, matches(".pred")[1])
    auc <- preds %>% 
      roc_auc(truth, matches(".pred")[1]) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      ) +
      theme(panel.grid = element_blank())
  }) %>% 
  imap(~ .x +
         labs(title = .y) +
         theme(plot.title = element_markdown(face = "bold"))) %>% 
  wrap_plots(widths = c(1, -1)) +
  plot_annotation(
    title = "ROC Curves for Sequential, Full Set Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_seq_full)
```

#### Sequential, Optimal

```{r}
#| label: fig-roc-curve-seq-opt
#| fig-cap: 'ROC Curves for Sequential, Optimal Model in Confirmation Set'
#| fig-height: 7
#| fig-width: 7
#| out-width: '100%'

roc_curve_seq_opt <- mod_seq_opt %>% 
  set_names(class_groups_seq) |> 
  list(conf_data_seq, conf_class_seq) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")%>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, matches(".pred")[1])
    auc <- preds %>% 
      roc_auc(truth, matches(".pred")[1]) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      ) +
      theme(panel.grid = element_blank())
  }) %>% 
  imap(~ .x +
         labs(title = .y) +
         theme(plot.title = element_markdown(face = "bold"))) %>% 
  wrap_plots(widths = c(1, -1)) +
  plot_annotation(
    title = "ROC Curves for Sequential, Optimal Set Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_seq_opt)
```

#### SMOTE-Random Forest, Full

```{r}
#| label: fig-roc-curve-smote-rf-full
#| fig-cap: 'ROC Curves for SMOTE-Random Forest, Full Set Model in Confirmation Set'
#| fig-height: 7
#| fig-width: 8
#| out-width: '100%'

preds <- mod_smote_rf_full %>% 
  predict(conf_data, type = "prob") %>%
  add_column(truth = factor(conf_class, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))
roc_df <- preds %>%
  roc_curve(truth, matches(".pred")) |> 
  mutate(.level = factor(.level, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))
auc <- preds %>%
  roc_auc(truth, matches(".pred")) %>% 
  pull(.estimate) %>% 
  number(accuracy = 0.001, prefix = "AUC = ")
roc_curve_smote_rf_full <-
  autoplot(roc_df) +
  labs(title = "ROC Curve for SMOTE-Random Forest, Full Set Model in Confirmation Set",
       subtitle = auc) +
  facet_wrap2(~ .level, strip = strip_themed(background_x = elem_list_rect(fill = class_palette))) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(face = "bold"))
print(roc_curve_smote_rf_full)
```

#### SMOTE-Random Forest, Optimal

```{r}
#| label: fig-roc-curve-smote-rf-opt
#| fig-cap: 'ROC Curves for SMOTE-Random Forest, Optimal Set Model in Confirmation Set'
#| fig-height: 7
#| fig-width: 8
#| out-width: '100%'

preds <- mod_smote_rf_opt |> 
  predict(conf_data, type = "prob") %>% 
  add_column(truth = factor(conf_class, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))
roc_df <- preds %>%
  roc_curve(truth, matches(".pred")) |> 
  mutate(.level = factor(.level, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))
auc <- preds %>%
  roc_auc(truth, matches(".pred")) %>% 
  pull(.estimate) %>% 
  number(accuracy = 0.001, prefix = "AUC = ")
roc_curve_smote_rf_opt <- 
  autoplot(roc_df) +
  labs(title = "ROC Curve for SMOTE-Random Forest, Optimal Set Model in Confirmation Set",
       subtitle = auc) +
  facet_wrap2(~ .level, strip = strip_themed(background_x = elem_list_rect(fill = class_palette))) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(face = "bold"))
print(roc_curve_smote_rf_opt)
```

#### Two-Step, Full

```{r}
#| label: fig-roc-curve-two-step-full
#| fig-cap: 'ROC Curves for Two-Step Full Model in Confirmation Set'
#| fig-height: 7
#| fig-width: 9
#| out-width: '100%'

roc_curve_two_step_full <- mod_two_step_full %>% 
  set_names(class_groups_two_step) %>% 
  list(conf_data_two_step, conf_class_two_step) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")
    prob_cols <- if (ncol(preds) == 2) names(preds)[1] else names(preds)
    preds <- preds %>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, all_of(prob_cols))
    auc <- preds %>% 
      roc_auc(truth, all_of(prob_cols)) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      ) +
      theme(panel.grid = element_blank(),
            strip.text = element_text(face = "bold"))
  }) |> 
  map_at(2, ~ .x +
           facet_wrap2(~ .level, strip = strip_themed(background_x = elem_list_rect(fill = class_palette[-1])))) |>
  imap(~ .x +
         labs(title = .y) +
         theme(plot.title = element_markdown(face = "bold"))) %>% 
  wrap_plots(widths = c(1, -1)) +
  plot_annotation(
    title = "ROC Curves for Two-Step, Full Set Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_two_step_full)
```

#### Two-Step, Optimal

```{r}
#| label: fig-roc-curve-two-step-optimal
#| fig-cap: 'ROC Curves for Two-Step Optimal Model in Confirmation Set'
#| fig-height: 7
#| fig-width: 9
#| out-width: '100%'

roc_curve_two_step_optimal <- mod_two_step_opt %>% 
  set_names(class_groups_two_step) %>% 
  list(conf_data_two_step, conf_class_two_step) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")
    prob_cols <- if (ncol(preds) == 2) names(preds)[1] else names(preds)
    preds <- preds %>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, all_of(prob_cols))
    auc <- preds %>% 
      roc_auc(truth, all_of(prob_cols)) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      ) +
      theme(panel.grid = element_blank(),
            strip.text = element_text(face = "bold"))
  }) |> 
  map_at(2, ~ .x +
           facet_wrap2(~ .level, strip = strip_themed(background_x = elem_list_rect(fill = class_palette[-1])))) |>
  imap(~ .x +
         labs(title = .y) +
         theme(plot.title = element_markdown(face = "bold"))) %>% 
  wrap_plots(widths = c(1, -1)) +
  plot_annotation(
    title = "ROC Curves for Two-Step, Optimal Set Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_two_step_optimal)
```
:::

### Validation Set

```{r}
#| label: tbl-val-eval
#| tbl-cap: 'Evaluation Metrics on Validation Set Model, SMOTE-Random Forest, Optimal Set'
#| tbl-pos: 'H'

# Validation set overall predictions
val_pred_overall <- data.frame(FileName = rownames(val_data),
                               Truth = val_class) %>%
  mutate(
    Prediction_smote_rf_opt = predict(mod_smote_rf_opt, val_data)[[".pred_class"]],
    across(c("Truth", matches("Prediction")), factor)
  ) %>% 
  pivot_longer(
    cols = matches("Prediction"),
    names_to = "Method",
    names_prefix = "Prediction_",
    values_to = "Prediction"
  ) %>% 
  mutate(across(c(Truth, Prediction),
                ~ factor(.x, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))),
         Method = "SMOTE-Random Forest, Optimal Set")

# Validation set overall metrics
val_eval_overall <- val_pred_overall %>%
  summarize(
    Accuracy = accuracy_vec(Truth, Prediction),
    Sensitivity = sensitivity_vec(Truth, Prediction),
    Specificity = specificity_vec(Truth, Prediction),
    `F1-Score` = f_meas_vec(Truth, Prediction),
    `Balanced Accuracy` = bal_accuracy_vec(Truth, Prediction),
    Kappa = kap_vec(Truth, Prediction)
  ) |> 
  pivot_longer(
    cols = where(is.numeric),
    names_to = "Metric",
    values_to = "Overall"
  )

# Validation set per-class metrics
val_eval_per_class <- val_pred_overall |>
  ova_metrics(Truth, Prediction, per_class_mset) |>
  mutate(
    Metric = case_match(
      .metric,
      "accuracy" ~ "Accuracy",
      "sensitivity" ~ "Sensitivity",
      "specificity" ~ "Specificity",
      "f_meas" ~ "F1-Score",
      "bal_accuracy" ~ "Balanced Accuracy",
      "kap" ~ "Kappa"
    ),
    .keep = "unused"
  ) |>
  pivot_wider(id_cols = Metric,
              names_from = class_group,
              values_from = .estimate)

# Combined all metrics
val_eval <-
  inner_join(val_eval_overall, val_eval_per_class, by = "Metric")

val_eval |> 
  kbl(booktabs = TRUE, linesep = "", digits = 3) |>
  kable_styling() |> 
  collapse_rows(columns = 1) |> 
  add_header_above(c(" " = 2, "Histotypes" = 5))
```

```{r}
#| label: fig-val-metrics-smote-rf
#| fig-cap: 'SMOTE-Random Forest Per-Class Metrics in Validation Set'
#| fig-width: 8
#| out-width: '100%'

val_metrics_top <- val_eval_per_class |> 
  pivot_longer(
    cols = where(is.numeric),
    names_to = "Class",
    names_ptypes = list(Class = factor(
      levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")
    )),
    values_to = "Value"
  ) |> 
  filter(Metric %in% c("F1-Score", "Balanced Accuracy", "Kappa")) |> 
  mutate(Metric = fct_inorder(Metric))
  
p <- ggplot(val_metrics_top) +
  geom_col(
    aes(x = Class, y = Value, fill = Class),
    show.legend = FALSE
  ) +
  geom_hline(yintercept = 0.8, color = "grey20", alpha = 0.5, linetype = "dashed") +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(breaks = seq(0, 1, 0.2)) +
  facet_wrap(vars(Metric), nrow = 1) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  ) +
  labs(
    y = "Metric Value",
    title = paste("SMOTE-Random Forest Per-Class Metrics in Validation Set")
  )
print(p)
```

```{r}
#| label: fig-val-conf-mat
#| fig-cap: 'Confusion Matrix for Validation Set Model'

val_conf_mat <- val_pred_overall %>%
  nest(.by = Method) %>%
  deframe() %>%
  imap(
    ~ conf_mat(.x, Truth, Prediction) %>%
      autoplot(type = "heatmap") +
      scale_fill_distiller(palette = "YlOrBr", direction = 1) +
      labs(title = .y) +
      theme(
        axis.ticks = element_blank(),
        axis.text.y = element_markdown(colour = rev(class_palette), margin = margin(r = -5)),
        axis.text.x = element_markdown(colour = class_palette, margin = margin(t = -3))
      )
  ) %>% 
  wrap_plots() +
  plot_annotation(title = "Confusion Matrix for Validation Set Model")

print(val_conf_mat)
```

```{r}
#| label: fig-val-roc-curve-smote-rf-opt
#| fig-cap: 'ROC Curves for SMOTE-Random Forest, Optimal Set Model in Validation Set'
#| fig-height: 7
#| fig-width: 8
#| out-width: '100%'

preds <- mod_smote_rf_opt |> 
  predict(val_data, type = "prob") |> 
  add_column(truth = factor(val_class, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))
roc_df <- preds %>%
  roc_curve(truth, matches(".pred")) |> 
  mutate(.level = factor(.level, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))
auc <- preds %>%
  roc_auc(truth, matches(".pred")) %>% 
  pull(.estimate) %>% 
  number(accuracy = 0.001, prefix = "AUC = ")
val_roc_curve_smote_rf_opt <- 
  autoplot(roc_df) +
  labs(title = "ROC Curve for SMOTE-Random Forest, Optimal Set Model in Validation Set",
       subtitle = auc) +
  facet_wrap2(~ .level, strip = strip_themed(background_x = elem_list_rect(fill = class_palette))) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(face = "bold"))
print(val_roc_curve_smote_rf_opt)
```

```{r}
#| label: fig-val-pred-protype
#| fig-cap: 'Subtype Prediction Summary among Predicted HGSC Samples'
#| fig-height: 7
#| fig-width: 10
#| out-width: '100%'

val_pred_hgsc_samples <- mod_smote_rf_opt |> 
  predict(val_data, type = "class") |> 
  bind_cols(val_data) |> 
  filter(.pred_class == "HGSC") |> 
  rownames_to_column("FileName")

val_data_protype <- cs3_norm |>
  pivot_longer(
    cols = where(is.numeric),
    names_to = "FileName",
    names_prefix = "X",
    values_to = "Exp"
  ) |>
  pivot_wider(id_cols = FileName,
              names_from = "Name",
              values_from = "Exp") |>
  semi_join(val_pred_hgsc_samples, by = "FileName") |>
  column_to_rownames("FileName")

val_pred_protype <- final_model |>
  predict(val_data_protype, type = "prob") |>
  as.data.frame() |>
  rownames_to_column("FileName") |>
  rowwise() |>
  mutate(prob = max(c_across(where(is.numeric))),
         entropy = entropy::entropy(c_across(matches("^C")), unit = "log2")) |>
  ungroup() |>
  bind_cols(Subtype = predict(final_model, val_data_protype, type = "class")) |> 
  add_count(Subtype) |> 
  mutate(median_entropy = median(entropy), .by = Subtype)

p1 <- ggplot(val_pred_protype, aes(x = Subtype, fill = Subtype)) +
  geom_text(aes(y = n, label = n),
            check_overlap = TRUE,
            nudge_y = 10) +
  geom_bar() +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  labs(y = "Count", title = "Distribution of Subtypes") +
  guides(fill = "none")

p2 <- ggplot(val_pred_protype, aes(x = Subtype, y = entropy, fill = Subtype)) +
  geom_boxplot() +
  geom_text(
    aes(
      y = median_entropy,
      label = number(median_entropy, prefix = "Median: ")
    ),
    check_overlap = TRUE,
    nudge_y = 0.1,
    size = 3
  ) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  labs(x = "Subtype", y = "Entropy (log base 2)", title = "Distribution of Entropy") +
  guides(fill = "none")

p3 <- ggplot(val_pred_protype, aes(x = prob, y = entropy, color = Subtype)) +
  geom_point(alpha = 0.7) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "inside",
    legend.position.inside = c(0.96, 0.95),
    legend.justification.inside = c(1, 1),
    legend.box.background = element_rect(color = "black", linewidth = 1),
    legend.key.spacing.y = unit(-3, "pt")
  ) +
  labs(x = "Probability",
       y = "Entropy (log base 2)",
       title = "Probability by Entropy")

p <- wrap_plots(list(A = p1, B = p2, C = p3),
                design = "AB
                          CC") +
  plot_annotation(
    title = "Subtype Prediction Summary among Predicted HGSC Samples",
    tag_levels = "A",
    theme = theme(plot.title = element_text(face = "bold"))
  )

print(p)
```
