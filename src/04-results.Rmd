# Results

```{r setup-04}
# Load packages and data
library(dplyr)
library(purrr)
library(tibble)
library(tidyr)
library(ggplot2)
library(forcats)
library(knitr)
library(kableExtra)
library(RankAggreg)
library(splendid)
library(magrittr)
library(DT)
library(plotly)
library(tidymodels)
library(themis)
library(here)
source(here("validation/cs_process_cohorts.R"))
source(here("src/funs.R"))

# Metrics
all_metrics_train <- readRDS(here("data/all_metrics_train.rds"))
per_class_metrics_seq <-
  readRDS(here("data/per_class_metrics_seq.rds"))
per_class_metrics_two_step <-
  readRDS(here("data/per_class_metrics_two_step.rds"))

# Models
all_models_seq <- readRDS(here("data/all_models_seq.rds"))
all_models_two_step <- readRDS(here("data/all_models_two_step.rds"))

# Overall Variable Importance
all_vi_seq <- readRDS(here("data/all_vi_seq.rds"))
all_vi_two_step <- readRDS(here("data/all_vi_two_step.rds"))

# Gene Optimization
gene_opt_all_metrics_seq <-
  readRDS(here("data/gene_opt_all_metrics_seq.rds"))
gene_opt_all_metrics_two_step <- 
  readRDS(here("data/gene_opt_all_metrics_two_step.rds"))

# PrOTYPE and SPOT genes
overlap_PrOTYPE <- intersect(names(train_data), cs5$Name)
overlap_SPOT <- intersect(names(train_data), coefmat[["Symbol"]])
overlap_all <- unique(c(overlap_PrOTYPE, overlap_SPOT))
```

We summarize cross-validated training performance of class metrics in the training set. The accuracy, F1-score, kappa, and G-mean are the metrics of interest. Workflows are ordered by their mean estimates across the outer folds of the nested CV for each metric.

## Training Set

```{r train-metrics}
all_metrics_train <- all_metrics_train %>%
  separate(wflow, c("Subsampling", "Algorithms"), remove = FALSE) %>%
  mutate(
    Subsampling = factor(Subsampling, levels = c("none", "down", "up", "smote", "hybrid")),
    Algorithms = factor(Algorithms, levels = c("rf", "svm", "xgb", "mr")),
    class_group = factor(
      class_group,
      levels = c("Overall", "HGSC", "CCOC", "LGSC", "ENOC", "MUC")
    )
  )
```

### Accuracy

```{r train-accuracy, fig.cap='Training Set Mean Accuracy'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_point(size = 3) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Accuracy",
       title = "Training Set Mean Accuracy")
print(p)
```

```{r train-accuracy-table}
overall_accuracy_train <-
  all_metrics_train %>% 
  filter(class_group == "Overall", .metric == "accuracy") %>% 
  mutate(
    mean_estimate = round(mean_estimate, digits = 3),
    mean_estimate = ifelse(
      mean_estimate == max(mean_estimate, na.rm = TRUE),
      cell_spec(mean_estimate, background = "#90ee90"),
      mean_estimate
    )
  ) %>% 
  arrange(Subsampling, Algorithms) %>% 
  pivot_wider(id_cols = Subsampling,
              names_from = Algorithms,
              values_from = mean_estimate)

kable(overall_accuracy_train,
      caption = "Training Set Mean Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-accuracy-class, fig.cap='Training Set Class-Specific Mean Accuracy', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_point(size = 3) +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Accuracy",
       title = "Training Set Class-Specific Mean Accuracy")
print(p)
```

```{r train-accuracy-class-table}
class_accuracy_train <-
  all_metrics_train %>% 
  filter(class_group != "Overall", .metric == "accuracy") %>% 
  rename(Histotype = class_group) %>% 
  mutate(
    mean_estimate = round(mean_estimate, digits = 3),
    mean_estimate = ifelse(
      mean_estimate == max(mean_estimate, na.rm = TRUE),
      cell_spec(mean_estimate, background = "#90ee90"),
      mean_estimate
    )
  ) %>% 
  arrange(Subsampling, Histotype, Algorithms) %>% 
  pivot_wider(id_cols = c(Subsampling, Histotype),
              names_from = Algorithms,
              values_from = mean_estimate)

kable(class_accuracy_train,
      caption = "Training Set Class-Specific Mean Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### F1-Score

```{r train-f1, fig.cap='Training Set Mean F1-Score'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "f_meas"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_point(size = 3) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "F1-Score",
       title = "Training Set Mean F1-Score")
print(p)
```

```{r train-f1-table}
overall_f1_train <-
  all_metrics_train %>% 
  filter(class_group == "Overall", .metric == "f_meas") %>% 
  mutate(
    mean_estimate = round(mean_estimate, digits = 3),
    mean_estimate = ifelse(
      mean_estimate == max(mean_estimate, na.rm = TRUE),
      cell_spec(mean_estimate, background = "#90ee90"),
      mean_estimate
    )
  ) %>% 
  arrange(Subsampling, Algorithms) %>% 
  pivot_wider(id_cols = Subsampling,
              names_from = Algorithms,
              values_from = mean_estimate)

kable(overall_f1_train,
      caption = "Training Set Mean F1-Score",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-f1-class, fig.cap='Training Set Class-Specific Mean F1-Score', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "f_meas"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_point(size = 3) +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "F1-Score",
       title = "Training Set Class-Specific Mean F1-Score")
print(p)
```

```{r train-f1-class-table}
class_f1_train <-
  all_metrics_train %>% 
  filter(class_group != "Overall", .metric == "f_meas") %>% 
  rename(Histotype = class_group) %>% 
  mutate(
    mean_estimate = round(mean_estimate, digits = 3),
    mean_estimate = ifelse(
      mean_estimate == max(mean_estimate, na.rm = TRUE),
      cell_spec(mean_estimate, background = "#90ee90"),
      mean_estimate
    )
  ) %>% 
  arrange(Subsampling, Histotype, Algorithms) %>% 
  pivot_wider(id_cols = c(Subsampling, Histotype),
              names_from = Algorithms,
              values_from = mean_estimate)

kable(class_f1_train,
      caption = "Cross-Validated Training Set Class-Specific Mean F1-Score",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Kappa

```{r train-kappa, fig.cap='Training Set Mean Kappa'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "kap"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_point(size = 3) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Mean Kappa")
print(p)
```

```{r train-kappa-table}
overall_kappa_train <-
  all_metrics_train %>% 
  filter(class_group == "Overall", .metric == "kap") %>% 
  mutate(
    mean_estimate = round(mean_estimate, digits = 3),
    mean_estimate = ifelse(
      mean_estimate == max(mean_estimate, na.rm = TRUE),
      cell_spec(mean_estimate, background = "#90ee90"),
      mean_estimate
    )
  ) %>% 
  arrange(Subsampling, Algorithms) %>% 
  pivot_wider(id_cols = Subsampling,
              names_from = Algorithms,
              values_from = mean_estimate)

kable(overall_kappa_train,
      caption = "Training Set Mean Kappa",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-kappa-class, fig.cap='Training Set Class-Specific Mean Kappa', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "kap"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_point(size = 3) +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Class-Specific Mean Kappa")
print(p)
```

```{r train-kappa-class-table}
class_kappa_train <-
  all_metrics_train %>% 
  filter(class_group != "Overall", .metric == "kap") %>% 
  rename(Histotype = class_group) %>% 
  mutate(
    mean_estimate = round(mean_estimate, digits = 3),
    mean_estimate = ifelse(
      mean_estimate == max(mean_estimate, na.rm = TRUE),
      cell_spec(mean_estimate, background = "#90ee90"),
      mean_estimate
    )
  ) %>% 
  arrange(Subsampling, Histotype, Algorithms) %>% 
  pivot_wider(id_cols = c(Subsampling, Histotype),
              names_from = Algorithms,
              values_from = mean_estimate)

kable(class_kappa_train,
      caption = "Training Set Class-Specific Mean Kappa",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### G-mean

```{r train-gmean, fig.cap='Training Set Mean G-mean'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "gmean"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_point(size = 3) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "G-mean",
       title = "Training Set Mean G-mean")
print(p)
```

```{r train-gmean-table}
overall_gmean_train <-
  all_metrics_train %>% 
  filter(class_group == "Overall", .metric == "gmean") %>% 
  mutate(
    mean_estimate = round(mean_estimate, digits = 3),
    mean_estimate = ifelse(
      mean_estimate == max(mean_estimate, na.rm = TRUE),
      cell_spec(mean_estimate, background = "#90ee90"),
      mean_estimate
    )
  ) %>% 
  arrange(Subsampling, Algorithms) %>% 
  pivot_wider(id_cols = Subsampling,
              names_from = Algorithms,
              values_from = mean_estimate)

kable(overall_gmean_train,
      caption = "Training Set Mean G-mean",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-gmean-class, fig.cap='Training Set Class-Specific Mean G-mean', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "gmean"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_point(size = 3) +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Class-Specific Mean G-mean")
print(p)
```

```{r train-gmean-class-table}
class_gmean_train <-
  all_metrics_train %>% 
  filter(class_group != "Overall", .metric == "gmean") %>% 
  rename(Histotype = class_group) %>% 
  mutate(
    mean_estimate = round(mean_estimate, digits = 3),
    mean_estimate = ifelse(
      mean_estimate == max(mean_estimate, na.rm = TRUE),
      cell_spec(mean_estimate, background = "#90ee90"),
      mean_estimate
    )
  ) %>% 
  arrange(Subsampling, Histotype, Algorithms) %>% 
  pivot_wider(id_cols = c(Subsampling, Histotype),
              names_from = Algorithms,
              values_from = mean_estimate)

kable(class_gmean_train,
      caption = "Training Set Class-Specific Mean G-mean",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

## Optimal Gene Sets

### Sequential Algorithm

```{r gene-seq, fig.height=8, fig.cap='Gene Optimization for Sequential Classifier'}
gene_seq_names <- all_vi_seq %>% 
  select(Name = Gene_Order) %>% 
  rowid_to_column("Genes")

dat_gene_seq <- gene_opt_all_metrics_seq %>% 
  filter(.metric == "f_meas") %>% 
  mutate(
    label = paste0(
      "Sequence: ",
      Sequence,
      ", Workflow: ",
      wflow,
      ", Measure: ",
      .metric)
  ) %>% 
  inner_join(gene_seq_names, by = "Genes")

p_gene_seq <-
  ggplot(dat_gene_seq, aes(x = Genes, y = mean_estimate, text = Name)) +
  facet_wrap(~ label, ncol = 1) +
  geom_point() +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for Sequential Classifier")
ggplotly(p_gene_seq)

n_genes_seq <- 27
genes_add_seq <- gene_seq_names %>%
  slice_min(order_by = Genes, n = n_genes_seq) %>% 
  pull(Name)
opt_genes_seq <- c(overlap_all, genes_add_seq)
```

In the sequential algorithm, sequences 1, 2, and 4 have relatively flat average F1-scores across the number of genes added. However, we can observe in sequence 3, the F1-score stabilizes at around 0.9 when we reach `r n_genes_seq` genes added, hence the optimal number of genes used will be n=`r length(overlap_all)`+`r n_genes_seq`=`r length(opt_genes_seq)` The added genes are: `r str_flatten_comma(genes_add_seq, last = " and ")`.

### Two-Step Algorithm

```{r gene-2s, fig.cap='Gene Optimization for Two-Step Classifier'}
gene_2s_names <- all_vi_two_step %>% 
  select(Name = Gene_Order) %>% 
  rowid_to_column("Genes")

dat_gene_2s <- gene_opt_all_metrics_two_step %>% 
  filter(.metric == "f_meas") %>% 
  mutate(
    label = paste0(
      "Sequence: ",
      Sequence,
      ", Workflow: ",
      wflow,
      ", Measure: ",
      .metric)
  ) %>% 
  inner_join(gene_seq_names, by = "Genes")

p_gene_2s <-
  ggplot(dat_gene_2s, aes(x = Genes, y = mean_estimate, text = Name)) +
  facet_wrap(~ label, ncol = 1) +
  geom_point() +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for Two-Step Classifier")
ggplotly(p_gene_2s)

n_genes_2s <- 12
genes_add_2s <- gene_2s_names %>%
  slice_min(order_by = Genes, n = n_genes_2s) %>% 
  pull(Name)
opt_genes_2s <- c(overlap_all, genes_add_2s)
```

Since the second step of the classifier fits a multinomial model, we use the macro F1-score as the measure to analyze gene entry. In the two-step classifier, we see that in Step 2, the F1-score stabilizes at around 0.85 when we reach `r n_genes_2s` added. The optimal number of genes used will be n=`r length(overlap_all)`+`r n_genes_2s`=`r length(opt_genes_2s)`. The added genes are: `r str_flatten_comma(genes_add_2s, last = " and ")`.

## Rank Aggregation

```{r per-class-metrics}
# Combine per-class metrics
per_class_metrics <- bind_rows(
  all_metrics_train,
  per_class_metrics_seq %>%
    add_column(wflow = "sequential", .before = 1),
  per_class_metrics_two_step %>%
    add_column(wflow = "two_step", .before = 1)
) %>% 
  filter(class_group != "Overall") %>% 
  mutate(class_group = factor(class_group,
                              levels = c("HGSC", "CCOC", "LGSC", "ENOC", "MUC")))

# Keep only workflows that predicted at least one case in each class
per_class_f1 <- per_class_metrics %>%
  filter(.metric == "f_meas") %>%
  select(wflow, class_group, mean_estimate) %>%
  arrange(class_group) %>% 
  pivot_wider(names_from = "class_group", values_from = "mean_estimate") %>%
  drop_na() %>%
  pivot_longer(-wflow, names_to = "class_group", values_to = "mean_estimate")

# Rank F1-measures
wflow_ranks <- per_class_f1 %>%
  group_by(class_group) %>%
  mutate(rank = factor(row_number(-mean_estimate))) %>%
  ungroup() %>%
  arrange(rank) %>%
  pivot_wider(id_cols = class_group,
              names_from = "rank",
              values_from = "wflow") %>%
  column_to_rownames("class_group") %>%
  as.matrix()

# Rank aggregation using genetic algorithm
wflow_top <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks,
  k = ncol(wflow_ranks),
  method = "GA",
  seed = 2022,
  verbose = FALSE
)) %>% 
  pluck("top.list") %>% 
  enframe(name = "rank", value = "wflow")

# F1-score summary
f1_summary <- per_class_f1 %>% 
  pivot_wider(names_from = class_group, values_from = mean_estimate) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 3))) %>% 
  inner_join(wflow_top, by = "wflow") %>% 
  arrange(rank) %>% 
  rename(Workflow = wflow, Rank = rank)

datatable(
  f1_summary,
  options = list(
    scrollX = TRUE,
    fixedColumns = TRUE,
    columnDefs = list(
      list(
        targets = 1,
        render = JS("$.fn.dataTable.render.ellipsis( 10 )")
      ),
      list(type = "natural", targets = 0)
    ),
    pageLength = 50
  ), 
  rownames = FALSE,
  caption = "F1-Score Summary by Workflow and Class",
  filter = "top",
  extensions = "FixedColumns",
  plugins = c("ellipsis", "natural")
)
```

The `r ncol(wflow_ranks)` workflows are ordered in the table by their aggregated ranks using the Genetic Algorithm. We see that the best performing methods involve the sequential and two-step algorithms.

### Top Workflows

We look at the per-class evaluation metrics of the top 4 workflows.

```{r metrics-top4, fig.cap='Top 4 Workflow Per-Class Evaluation Metrics'}
wflow_top4 <- wflow_top %>% 
  head(4)

metrics_top4 <- per_class_metrics %>% 
  inner_join(wflow_top4, by = "wflow") %>% 
  mutate(wflow = fct_reorder(wflow, rank)) %>% 
  arrange(rank)

p <- 
  ggplot(metrics_top4, aes(x = wflow, y = mean_estimate, color = .metric)) +
  geom_point(position = position_jitter(width = 0.1)) +
  facet_wrap(vars(class_group), ncol = 2) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = "Workflow",
    y = "Metric",
    color = "Metric",
    title = "Top 4 Workflow Per-Class Evaluation Metrics"
  )
print(p)
```

```{r metrics-f1-top4, fig.cap='Top 4 Workflow Per-Class F1-Scores'}
metrics_f1_top4 <- metrics_top4 %>% 
  filter(.metric == "f_meas")

p <- 
  ggplot(metrics_f1_top4,
         aes(x = class_group, y = mean_estimate, color = wflow)) +
  geom_point(position = position_jitter(width = 0.1)) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = "Class",
    y = "Metric",
    color = "Workflow",
    title = "Top 4 Workflow Per-Class F1-Scores"
  )
print(p)
```

Misclassified cases from a previous step of the sequence of classifiers are not included in subsequent steps of the training set CV folds. Thus, we cannot piece together the test set predictions from the sequential and two-step algorithms to obtain overall metrics.

## Test Set Performance

Now we'd like to see how our best methods perform in the confirmation and validation sets. The class-specific F1-scores will be used.

The top 2 methods are:

- **sequential**: sequential algorithm with hybrid subsampling at every step. The sequence of algorithms used are:
  - HGSC vs. non-HGSC using random forest
  - CCOC vs. non-CCOC using support vector machine
  - LGSC vs. non-LGSC using support vector machine
  - ENOC vs. MUC using regularized multinomial regression
- **two_step**: two-step algorithm with hybrid subsampling at both steps. The sequence of algorithms used are:
  - HGSC vs. non-HGSC using random forest
  - CCOC vs. ENOC vs. MUC vs. LGSC support vector machine
  
We can test 2 additional methods by using either the full set of genes or the optimal set of genes for both of these methods.

```{r top-models}
# Two-step data
two_step_data <- readRDS(here("data", "two_step_data.rds"))
two_step_class <- readRDS(here("data", "two_step_class.rds"))

# Two-step full model
set.seed(2024)
mod_two_step_full <- list(all_models_two_step,
                          two_step_data,
                          two_step_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>% fit(data = data)
  })

# Two-step optimal gene list model
set.seed(2024)
mod_two_step_opt <- list(all_models_two_step,
                         two_step_data,
                         two_step_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>%
      update_recipe(pluck(., "pre", "actions", "recipe", "recipe") %>%
                      step_select(c(all_of(!!opt_genes_2s), class),
                                  skip = TRUE)) %>%
      fit(data = data)
  })

# sequential data
seq_data <- readRDS(here("data", "seq_data.rds"))
seq_class <- readRDS(here("data", "seq_class.rds"))
seq_wflows <- readRDS(here("data", "seq_wflows.rds"))

# Sequential full model
set.seed(2024)
mod_seq_full <- list(all_models_seq,
                     seq_data,
                     seq_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>% fit(data = data)
  })

# Sequential optimal gene list model
set.seed(2024)
mod_seq_opt <- list(all_models_seq,
                    seq_data,
                    seq_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>%
      update_recipe(pluck(., "pre", "actions", "recipe", "recipe") %>%
                      step_select(c(all_of(!!opt_genes_seq), class),
                                  skip = TRUE)) %>%
      fit(data = data)
  })

# Per-class metric set
gmean <- new_class_metric(gmean, direction = "maximize")
per_class_mset <-
  metric_set(accuracy, f_meas, kap, gmean)
```

### Confirmation Set

```{r confirmation-set}
# Load data
conf_class <- readRDS(here("data", "conf_class.rds"))
conf_data <- readRDS(here("data", "conf_data.rds"))
```

```{r conf-eval-overall}
# Confirmation set overall predictions
conf_pred_overall <- data.frame(FileName = rownames(conf_data),
                                Truth = conf_class) %>%
  mutate(
    c(
      p_2s_full = mod_two_step_full,
      p_2s_opt = mod_two_step_opt,
      p_seq_full = mod_seq_full,
      p_seq_opt = mod_seq_opt
    ) %>%
      map(~ predict(., conf_data)[[".pred_class"]]) %>%
      bind_cols()
  ) %>% 
  rename_with(~ gsub("(.*)\\..*_(.*)", "\\1_\\2", .), where(is.factor)) %>% 
  mutate(
    across(matches("p"), as.character),
    Prediction_two_step_full = ifelse(!grepl("non", p_2s_full_s1),
                                      p_2s_full_s1, p_2s_full_s2),
    Prediction_two_step_optimal = ifelse(!grepl("non", p_2s_opt_s1),
                                         p_2s_opt_s1, p_2s_opt_s2), 
    Prediction_sequential_full = case_when(
      !grepl("non", p_seq_full_s1) ~ p_seq_full_s1,
      !grepl("non", p_seq_full_s2) ~ p_seq_full_s2,
      !grepl("non", p_seq_full_s3) ~ p_seq_full_s3,
      .default = p_seq_full_s4
    ),
    Prediction_sequential_optimal = case_when(
      !grepl("non", p_seq_opt_s1) ~ p_seq_opt_s1,
      !grepl("non", p_seq_opt_s2) ~ p_seq_opt_s2,
      !grepl("non", p_seq_opt_s3) ~ p_seq_opt_s3,
      .default = p_seq_opt_s4
    ),
    across(c("Truth", matches("Prediction")), factor)
  ) %>%
  select(-starts_with("p", ignore.case = FALSE)) %>%
  pivot_longer(
    cols = matches("Prediction"),
    names_to = "Method",
    names_prefix = "Prediction_",
    values_to = "Prediction"
  )

# Confirmation set overall metrics
conf_eval_overall <- conf_pred_overall %>%
  group_by(Method) %>%
  summarize(
    accuracy = accuracy_vec(Truth, Prediction),
    f1 = f_meas_vec(Truth, Prediction),
    kappa = kap_vec(Truth, Prediction),
    gmean = gmean_vec(Truth, Prediction)
  )

kable(conf_eval_overall,
      caption = "Overall Evaluation Metrics on Confirmation Set Models",
      digits = 3) %>%
  kable_styling()
```

```{r conf-eval-per-class}
conf_eval_per_class <- conf_pred_overall %>% 
  mutate(
    pred_class_ova = map(Prediction, ~ {
      ifelse(levels(Prediction) %in% .x, as.character(.x), "class_0") %>%
        set_names(paste0(".pred_class_", levels(Prediction)))
    }),
    class_ova = map(Truth, ~ {
      ifelse(levels(Truth) %in% .x, as.character(.x), "class_0") %>%
        set_names(paste0("class_", levels(Truth)))
    })
  ) %>% 
  unnest_wider(col = c(pred_class_ova, class_ova)) %>%
  pivot_longer(
    matches("^.pred_class_.*"),
    names_to = ".pred_class_group",
    names_prefix = ".pred_class_",
    values_to = ".pred_class_value"
  ) %>%
  pivot_longer(
    matches("^class_.*"),
    names_to = "class_group",
    names_prefix = "class_",
    values_to = "class_value"
  ) %>%
  filter(class_group == .pred_class_group) %>%
  nest(.by = c(Method, class_group)) %>% 
  mutate(
    data = data %>%
      map(~ mutate(.x, across(
        matches("class_value"),
        ~ factor(.x, levels = unique(c(
          .pred_class_group, "class_0"
        )))
      ))) %>%
      map(per_class_mset, truth = class_value, estimate = .pred_class_value) %>%
      suppressWarnings()
  ) %>% 
  unnest(cols = data) %>% 
  select(-.estimator) %>% 
  mutate(
    .metric = fct_recode(.metric, f1 = "f_meas", kappa = "kap"),
    class_group = factor(class_group, levels = c("HGSC", "CCOC", "LGSC", "ENOC", "MUC"))
  ) %>% 
  rename(Metric = .metric) %>% 
  arrange(class_group) %>%
  pivot_wider(names_from = "class_group", values_from = ".estimate")

kable(conf_eval_per_class,
      caption = "Per-Class Eevaluation Metrics on Confirmation Set Model",
      digits = 3) %>% 
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Histotypes" = 5))
```

### Validation Set

```{r validation-set}
# Load data
val_class <- readRDS(here("data", "val_class.rds"))
val_data <- readRDS(here("data", "val_data.rds"))
```

```{r val-eval-overall}
# Validation set overall predictions
val_pred_overall <- data.frame(FileName = rownames(val_data),
                               Truth = val_class) %>%
  mutate(
    c(p_2s_opt = mod_two_step_opt) %>%
      map(~ predict(., val_data)[[".pred_class"]]) %>%
      bind_cols()
  ) %>% 
  rename_with(~ gsub("(.*)\\..*_(.*)", "\\1_\\2", .), where(is.factor)) %>% 
  
  mutate(
    across(matches("p"), as.character),
    Prediction_two_step_optimal = ifelse(!grepl("non", p_2s_opt_s1),
                                         p_2s_opt_s1, p_2s_opt_s2),
    across(c("Truth", matches("Prediction")), factor)
  ) %>%
  select(-starts_with("p", ignore.case = FALSE)) %>%
  pivot_longer(
    cols = matches("Prediction"),
    names_to = "Method",
    names_prefix = "Prediction_",
    values_to = "Prediction"
  )

# Validation set overall metrics
val_eval_overall <- val_pred_overall %>%
  group_by(Method) %>%
  summarize(
    accuracy = accuracy_vec(Truth, Prediction),
    f1 = f_meas_vec(Truth, Prediction),
    kappa = kap_vec(Truth, Prediction),
    gmean = gmean_vec(Truth, Prediction)
  )

kable(val_eval_overall,
      caption = "Overall Evaluation Metrics on Validation Set Model",
      digits = 3) %>% 
  kable_styling()
```

```{r val-eval-per-class}
val_eval_per_class <- val_pred_overall %>% 
  mutate(
    pred_class_ova = map(Prediction, ~ {
      ifelse(levels(Prediction) %in% .x, as.character(.x), "class_0") %>%
        set_names(paste0(".pred_class_", levels(Prediction)))
    }),
    class_ova = map(Truth, ~ {
      ifelse(levels(Truth) %in% .x, as.character(.x), "class_0") %>%
        set_names(paste0("class_", levels(Truth)))
    })
  ) %>% 
  unnest_wider(col = c(pred_class_ova, class_ova)) %>%
  pivot_longer(
    matches("^.pred_class_.*"),
    names_to = ".pred_class_group",
    names_prefix = ".pred_class_",
    values_to = ".pred_class_value"
  ) %>%
  pivot_longer(
    matches("^class_.*"),
    names_to = "class_group",
    names_prefix = "class_",
    values_to = "class_value"
  ) %>%
  filter(class_group == .pred_class_group) %>%
  nest(.by = c(Method, class_group)) %>% 
  mutate(
    data = data %>%
      map(~ mutate(.x, across(
        matches("class_value"),
        ~ factor(.x, levels = unique(c(
          .pred_class_group, "class_0"
        )))
      ))) %>%
      map(per_class_mset, truth = class_value, estimate = .pred_class_value) %>%
      suppressWarnings()
  ) %>% 
  unnest(cols = data) %>% 
  select(-.estimator) %>% 
  mutate(
    .metric = fct_recode(.metric, f1 = "f_meas", kappa = "kap"),
    class_group = factor(class_group, levels = c("HGSC", "CCOC", "LGSC", "ENOC", "MUC"))
  ) %>% 
  rename(Metric = .metric) %>% 
  arrange(class_group) %>%
  pivot_wider(names_from = "class_group", values_from = ".estimate")

kable(val_eval_per_class,
      caption = "Per-Class Eevaluation Metrics on Validation Set Model",
      digits = 3) %>% 
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Histotypes" = 5))
```
