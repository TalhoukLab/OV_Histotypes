# Results

```{r setup-04}
# Load packages and data
library(dplyr)
library(purrr)
library(tibble)
library(tidyr)
library(ggplot2)
library(forcats)
library(knitr)
library(kableExtra)
library(RankAggreg)
library(splendid)
library(magrittr)
library(DT)
library(plotly)
library(patchwork)
library(tidymodels)
library(themis)
library(here)
source(here("validation/cs_process_cohorts.R"))
source(here("src/funs.R"))

# Training data
train_data <- readRDS(here("data/train_data.rds"))
train_class <- readRDS(here("data/train_class.rds"))

# Metrics
all_metrics_train <- readRDS(here("data/all_metrics_train.rds"))
per_class_metrics_seq <-
  readRDS(here("data/per_class_metrics_seq.rds"))
per_class_metrics_two_step <-
  readRDS(here("data/per_class_metrics_two_step.rds"))

# Models
smote_rf_model <- readRDS(here("data/wflow_smote_rf_model_train.rds"))
all_models_seq <- readRDS(here("data/all_models_seq.rds"))
all_models_two_step <- readRDS(here("data/all_models_two_step.rds"))

# Overall Variable Importance
all_vi_train <- readRDS(here("data/all_vi_train.rds"))
all_vi_seq <- readRDS(here("data/all_vi_seq.rds"))
all_vi_two_step <- readRDS(here("data/all_vi_two_step.rds"))

# Gene Optimization
# gene_opt_all_metrics_seq <-
#   readRDS(here("data/gene_opt_all_metrics_seq.rds"))
# gene_opt_all_metrics_two_step <- 
#   readRDS(here("data/gene_opt_all_metrics_two_step.rds"))
gene_opt_all_metrics_conf <- 
  readRDS(here("data/gene_opt_all_metrics_conf.rds"))
gene_opt_all_metrics_conf_seq <-
  readRDS(here("data/gene_opt_all_metrics_conf_seq.rds"))
gene_opt_all_metrics_conf_two_step <-
  readRDS(here("data/gene_opt_all_metrics_conf_two_step.rds"))

# PrOTYPE and SPOT genes
coefmat <- readRDS(here("data/coefmat.rds"))
overlap_PrOTYPE <- intersect(names(train_data), cs5$Name)
overlap_SPOT <- intersect(names(train_data), coefmat[["Symbol"]])
overlap_all <- unique(c(overlap_PrOTYPE, overlap_SPOT))
```

We summarize cross-validated training performance of class metrics in the training set. The accuracy, F1-score, and kappa, are the metrics of interest. Workflows are ordered by their mean estimates across the outer folds of the nested CV for each metric.

## Training Set

```{r train-metrics}
all_metrics_train <- all_metrics_train %>%
  separate(wflow, c("Subsampling", "Algorithms"), remove = FALSE) %>%
  mutate(
    Subsampling = factor(Subsampling, levels = c("none", "down", "up", "smote", "hybrid")),
    Algorithms = factor(Algorithms, levels = c("rf", "svm", "xgb", "mr")),
    class_group = factor(
      class_group,
      levels = c("Overall", "HGSC", "CCOC", "ENOC", "LGSC", "MUC")
    )
  ) %>% 
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(wflow, class_group, .metric)
  ) %>% 
  mutate(
    undefined = case_when(any(is.nan(mean_estimate)) ~ "all",
                          any(is.na(.estimate)) ~ "some",
                          .default = "none"),
    .by = c(wflow, .metric)
  )
```

### Accuracy

```{r train-accuracy, fig.cap='Training Set Mean Accuracy'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Accuracy",
       title = "Training Set Mean Accuracy")
print(p)
```

```{r train-accuracy-table}
overall_accuracy_train <- summarize_metrics(all_metrics_train,
                                            metric = "accuracy",
                                            per_class = FALSE)

kable(overall_accuracy_train,
      caption = "Training Set Mean Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-accuracy-class, fig.cap='Training Set Class-Specific Mean Accuracy', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Accuracy",
       title = "Training Set Class-Specific Mean Accuracy")
print(p)
```

```{r train-accuracy-class-table}
class_accuracy_train <- summarize_metrics(all_metrics_train,
                                          metric = "accuracy",
                                          per_class = TRUE)
kable(class_accuracy_train,
      caption = "Training Set Class-Specific Mean Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Sensitivity

```{r train-sens, fig.cap='Training Set Mean Sensitivity'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "sensitivity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Sensitivity",
       title = "Training Set Mean Sensitivity")
print(p)
```

```{r train-sens-table}
overall_sens_train <- summarize_metrics(all_metrics_train,
                                        metric = "sensitivity",
                                        per_class = FALSE)
kable(overall_sens_train,
      caption = "Training Set Mean Sensitivity",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-sens-class, fig.cap='Training Set Class-Specific Mean Sensitivity', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "sensitivity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Sensitivity",
       title = "Training Set Class-Specific Mean Sensitivity")
print(p)
```

```{r train-sens-class-table}
class_sens_train <- summarize_metrics(all_metrics_train,
                                      metric = "sensitivity",
                                      per_class = TRUE)
kable(class_sens_train,
      caption = "Cross-Validated Training Set Class-Specific Mean Sensitivity",
      escape = FALSE) %>%
  kable_styling() %>%
  collapse_rows(columns = 1) %>%
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Specificity

```{r train-spec, fig.cap='Training Set Mean Specificity'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "specificity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Specificity",
       title = "Training Set Mean Specificity")
print(p)
```

```{r train-spec-table}
overall_spec_train <- summarize_metrics(all_metrics_train,
                                        metric = "specificity",
                                        per_class = FALSE)
kable(overall_spec_train,
      caption = "Training Set Mean Specificity",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-spec-class, fig.cap='Training Set Class-Specific Mean Specificity', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "specificity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Specificity",
       title = "Training Set Class-Specific Mean Specificity")
print(p)
```

```{r train-spec-class-table}
class_spec_train <- summarize_metrics(all_metrics_train,
                                      metric = "specificity",
                                      per_class = TRUE)
kable(class_spec_train,
      caption = "Cross-Validated Training Set Class-Specific Mean Specificity",
      escape = FALSE) %>%
  kable_styling() %>%
  collapse_rows(columns = 1) %>%
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### F1-Score

```{r train-f1, fig.cap='Training Set Mean F1-Score'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "f_meas"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "F1-Score",
       title = "Training Set Mean F1-Score")
print(p)
```

```{r train-f1-table}
overall_f1_train <- summarize_metrics(all_metrics_train,
                                      metric = "f_meas",
                                      per_class = FALSE)
kable(overall_f1_train,
      caption = "Training Set Mean F1-Score",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-f1-class, fig.cap='Training Set Class-Specific Mean F1-Score', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "f_meas"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "F1-Score",
       title = "Training Set Class-Specific Mean F1-Score")
print(p)
```

```{r train-f1-class-table}
class_f1_train <- summarize_metrics(all_metrics_train,
                                    metric = "f_meas",
                                    per_class = TRUE)
kable(class_f1_train,
      caption = "Cross-Validated Training Set Class-Specific Mean F1-Score",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Balanced Accuracy

```{r train-bal-accuracy, fig.cap='Training Set Mean Balanced Accuracy'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "bal_accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Balanced Accuracy",
       title = "Training Set Mean Balanced Accuracy")
print(p)
```

```{r train-bal-accuracy-table}
overall_bal_accuracy_train <- summarize_metrics(all_metrics_train,
                                                metric = "bal_accuracy",
                                                per_class = FALSE)

kable(overall_bal_accuracy_train,
      caption = "Training Set Mean Balanced Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-bal-accuracy-class, fig.cap='Training Set Class-Specific Mean Balanced Accuracy', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "bal_accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Balanced Accuracy",
       title = "Training Set Class-Specific Mean Balanced Accuracy")
print(p)
```

```{r train-bal-accuracy-class-table}
class_bal_accuracy_train <- summarize_metrics(all_metrics_train,
                                              metric = "bal_accuracy",
                                              per_class = TRUE)
kable(class_bal_accuracy_train,
      caption = "Training Set Class-Specific Mean Balanced Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Kappa

```{r train-kappa, fig.cap='Training Set Mean Kappa'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "kap"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Mean Kappa")
print(p)
```

```{r train-kappa-table}
overall_kappa_train <- summarize_metrics(all_metrics_train,
                                         metric = "kap",
                                         per_class = FALSE)
kable(overall_kappa_train,
      caption = "Training Set Mean Kappa",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-kappa-class, fig.cap='Training Set Class-Specific Mean Kappa', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "kap"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Class-Specific Mean Kappa")
print(p)
```

```{r train-kappa-class-table}
class_kappa_train <- summarize_metrics(all_metrics_train,
                                       metric = "kap",
                                       per_class = TRUE)
kable(class_kappa_train,
      caption = "Training Set Class-Specific Mean Kappa",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### G-mean

**DEPRECATED**

```{r train-gmean, fig.cap='Training Set Mean G-mean'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "gmean"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "G-mean",
       title = "Training Set Mean G-mean")
print(p)
```

```{r train-gmean-table}
overall_gmean_train <- summarize_metrics(all_metrics_train,
                                         metric = "gmean",
                                         per_class = FALSE)
kable(overall_gmean_train,
      caption = "Training Set Mean G-mean",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-gmean-class, fig.cap='Training Set Class-Specific Mean G-mean', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "gmean"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Class-Specific Mean G-mean")
print(p)
```

```{r train-gmean-class-table}
class_gmean_train <- summarize_metrics(all_metrics_train,
                                       metric = "gmean",
                                       per_class = TRUE)
kable(class_gmean_train,
      caption = "Training Set Class-Specific Mean G-mean",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

## Rank Aggregation

Multi-step methods:

- **sequential**: sequential algorithm sequence of subsampling methods and algorithms used are:
  - HGSC vs. non-HGSC using SMOTE subsampling and random forest
  - CCOC vs. non-CCOC using hybrid subsampling and XGBoost
  - ENOC vs. non-ENOC using upsampling and support vector machine
  - LGSC vs. MUC using hybrid subsampling and regularized multinomial regression
- **two_step**: two-step algorithm sequence of subsampling methods and algorithms used are:
  - HGSC vs. non-HGSC using SMOTE subsampling and random forest
  - CCOC vs. ENOC vs. MUC vs. LGSC using hybrid subsampling and support vector machine
  

We conduct rank aggregation using a two-stage nested appraoch:

1. First we rank aggregate the per-class metrics for F1-score, balanced accuracy and kappa.
2. Then we take the aggregated lists from the three metrics and perform a final rank aggregation.
3. The top workflows from the final rank aggregation are used for gene optimization in the confirmation set 
  
### Across Classes

```{r per-class-metrics}
# Add lower, upper, undefined columns for per-class sequential, two_step
per_class_metrics_seq <- per_class_metrics_seq |>
  mutate(class_group = factor(class_group, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))) |>
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(class_group, .metric)
  ) |>
  mutate(undefined = case_when(any(is.nan(mean_estimate)) ~ "all", any(is.na(.estimate)) ~ "some", .default = "none"),
         .by = .metric) |>
  add_column(wflow = "sequential", .before = 1)

per_class_metrics_two_step <- per_class_metrics_two_step |>
  mutate(class_group = factor(class_group, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))) |>
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(class_group, .metric)
  ) |>
  mutate(undefined = case_when(any(is.nan(mean_estimate)) ~ "all", any(is.na(.estimate)) ~ "some", .default = "none"),
         .by = .metric) |>
  add_column(wflow = "two_step", .before = 1)

# Combine all per-class metrics
per_class_metrics <- bind_rows(all_metrics_train,
                               per_class_metrics_seq,
                               per_class_metrics_two_step) |>
  filter(class_group != "Overall") |>
  droplevels()

# Keep only workflows that predicted at least one case in each class
per_class_metrics_comp <- per_class_metrics %>%
  distinct(pick(-fold_id, -.estimate)) %>%
  select(wflow, .metric, class_group, mean_estimate) %>%
  arrange(class_group) %>%
  pivot_wider(names_from = "class_group", values_from = "mean_estimate") %>%
  drop_na() %>%
  pivot_longer(cols = where(is.numeric),
               names_to = "class_group",
               values_to = "mean_estimate")
```

```{r rank-agg-f1}
# Select ranking metric
per_class_f1 <- filter(per_class_metrics_comp, .metric == "f_meas")

# Rank metric
wflow_ranks_f1 <- per_class_f1 %>%
  mutate(rank = factor(row_number(-mean_estimate)), .by = class_group) %>%
  arrange(rank) %>%
  pivot_wider(id_cols = class_group,
              names_from = "rank",
              values_from = "wflow") %>%
  column_to_rownames("class_group") %>%
  as.matrix()

# Rank aggregation of metrics using genetic algorithm
wflow_rank_agg_f1 <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks_f1,
  k = ncol(wflow_ranks_f1),
  method = "GA",
  seed = 2025,
  verbose = FALSE,
  maxIter = 1e4
)) %>% 
  pluck("top.list") %>% 
  enframe(name = "Rank", value = "Workflow")

# Metric summary with rank aggregation
f1_summary <- per_class_f1 |>
  rename(Workflow = wflow) |>
  mutate(mean_estimate = round(mean_estimate, digits = 3)) |>
  inner_join(wflow_rank_agg_f1, by = "Workflow") |>
  pivot_wider(id_cols = c(Workflow, Rank),
              names_from = class_group,
              values_from = mean_estimate) |> 
  arrange(Rank)

# Interactive table showing metrics used in rank aggregation
datatable(
  f1_summary,
  options = list(
    scrollX = TRUE,
    fixedColumns = TRUE,
    columnDefs = list(
      list(
        targets = 1,
        render = JS("$.fn.dataTable.render.ellipsis( 10 )")
      ),
      list(type = "natural", targets = 0)
    ),
    pageLength = 50
  ), 
  rownames = FALSE,
  caption = "F1-Score Rank Aggregation Summary",
  filter = "top",
  extensions = "FixedColumns",
  plugins = c("ellipsis", "natural")
)
```

```{r rank-agg-bal-acc}
# Select ranking metric
per_class_bal_acc <- filter(per_class_metrics_comp, .metric == "bal_accuracy")

# Rank metric
wflow_ranks_bal_acc <- per_class_bal_acc %>%
  mutate(rank = factor(row_number(-mean_estimate)), .by = class_group) %>%
  arrange(rank) %>%
  pivot_wider(id_cols = class_group,
              names_from = "rank",
              values_from = "wflow") %>%
  column_to_rownames("class_group") %>%
  as.matrix()

# Rank aggregation of metrics using genetic algorithm
wflow_rank_agg_bal_acc <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks_bal_acc,
  k = ncol(wflow_ranks_bal_acc),
  method = "GA",
  seed = 2025,
  verbose = FALSE,
  maxIter = 1e5
)) %>% 
  pluck("top.list") %>% 
  enframe(name = "Rank", value = "Workflow")

# Metric summary with rank aggregation
bal_acc_summary <- per_class_bal_acc |>
  rename(Workflow = wflow) |>
  mutate(mean_estimate = round(mean_estimate, digits = 3)) |>
  inner_join(wflow_rank_agg_bal_acc, by = "Workflow") |>
  pivot_wider(id_cols = c(Workflow, Rank),
              names_from = class_group,
              values_from = mean_estimate) |> 
  arrange(Rank)

# Interactive table showing metrics used in rank aggregation
datatable(
  bal_acc_summary,
  options = list(
    scrollX = TRUE,
    fixedColumns = TRUE,
    columnDefs = list(
      list(
        targets = 1,
        render = JS("$.fn.dataTable.render.ellipsis( 10 )")
      ),
      list(type = "natural", targets = 0)
    ),
    pageLength = 50
  ), 
  rownames = FALSE,
  caption = "Balanced Accuracy Rank Aggregation Summary",
  filter = "top",
  extensions = "FixedColumns",
  plugins = c("ellipsis", "natural")
)
```

```{r rank-agg-kap}
# Select ranking metric
per_class_kap <- filter(per_class_metrics_comp, .metric == "kap")

# Rank metric
wflow_ranks_kap <- per_class_kap %>%
  mutate(rank = factor(row_number(-mean_estimate)), .by = class_group) %>%
  arrange(rank) %>%
  pivot_wider(id_cols = class_group,
              names_from = "rank",
              values_from = "wflow") %>%
  column_to_rownames("class_group") %>%
  as.matrix()

# Rank aggregation of metrics using genetic algorithm
wflow_rank_agg_kap <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks_kap,
  k = ncol(wflow_ranks_kap),
  method = "GA",
  seed = 2025,
  verbose = FALSE,
  maxIter = 1e4
)) %>% 
  pluck("top.list") %>% 
  enframe(name = "Rank", value = "Workflow")

# Metric summary with rank aggregation
kap_summary <- per_class_kap |>
  rename(Workflow = wflow) |>
  mutate(mean_estimate = round(mean_estimate, digits = 3)) |>
  inner_join(wflow_rank_agg_kap, by = "Workflow") |>
  pivot_wider(id_cols = c(Workflow, Rank),
              names_from = class_group,
              values_from = mean_estimate) |> 
  arrange(Rank)

# Interactive table showing metrics used in rank aggregation
datatable(
  kap_summary,
  options = list(
    scrollX = TRUE,
    fixedColumns = TRUE,
    columnDefs = list(
      list(
        targets = 1,
        render = JS("$.fn.dataTable.render.ellipsis( 10 )")
      ),
      list(type = "natural", targets = 0)
    ),
    pageLength = 50
  ), 
  rownames = FALSE,
  caption = "Kappa Rank Aggregation Summary",
  filter = "top",
  extensions = "FixedColumns",
  plugins = c("ellipsis", "natural")
)
```

### Across Metrics

```{r rank-agg-comp}
rank_agg_comp <- list(F1 = wflow_rank_agg_f1,
                      `Balanced Accuracy` = wflow_rank_agg_bal_acc,
                      Kappa = wflow_rank_agg_kap) |>
  bind_rows(.id = "Metric") |>
  pivot_wider(names_from = "Metric", values_from = "Workflow")

rank_agg_comp |> 
  kbl(caption = "Rank Aggregation Comparison of Metrics Used")
```

```{r rank-agg-metrics}
# Rank aggregation of aggregated metrics using GA
wflow_ranks_comp <- rank_agg_comp |> 
  drop_na() |> 
  pivot_longer(cols = where(is.character), names_to = "Metric", values_to = "Workflow") |> 
  pivot_wider(names_from = Rank, values_from = Workflow) |> 
  column_to_rownames("Metric") |> 
  as.matrix()

# Take top 5 workflows
wflow_rank_agg_comp <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks_comp,
  k = 5,
  method = "GA",
  seed = 2025,
  verbose = FALSE
)) |>
  pluck("top.list") %>%
  enframe(name = "Rank", value = "Workflow")
```

### Top Workflows

We look at the per-class evaluation metrics of the top `r nrow(wflow_rank_agg_comp)` workflows.

```{r metrics-top-by-metric, fig.cap=paste('Top', nrow(wflow_rank_agg_comp), 'Workflow Per-Class Evaluation Metrics by Metric'), fig.height=7, out.width='100%'}
p <-
  ggplot(metrics_top) +
  geom_pointrange(
    aes(
      x = Workflow,
      y = mean_estimate,
      ymin = lower,
      ymax = upper,
      color = class_group
    ),
    position = position_jitter(width = 0.3),
    fatten = 1
  ) +
  scale_color_brewer(palette = "Dark2") + 
  facet_wrap(vars(.metric), ncol = 1, scales = "free_y") +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  ) +
  labs(
    x = "Workflow",
    y = "Metric Value",
    color = "Class",
    title = paste("Top", nrow(wflow_rank_agg_comp), "Workflow Per-Class Evaluation Metrics by Metric")
  )
print(p)
```

Misclassified cases from a previous step of the sequence of classifiers are not included in subsequent steps of the training set CV folds. Thus, we cannot piece together the test set predictions from the sequential and two-step algorithms to obtain overall metrics.

## Optimal Gene Sets

### Sequential Algorithm

```{r gene-seq, fig.height=8, fig.cap='Gene Optimization for Sequential Classifier'}
gene_seq_names <- all_vi_seq %>% 
  rename(Name = Variable) %>% 
  rowid_to_column("Genes") %>% 
  add_row(Genes = 0L, Name = "Base", .before = 1)

dat_gene_seq <- gene_opt_all_metrics_conf_seq %>% 
  filter(.metric == "f_meas", class_group == "Overall") %>% 
  mutate(
    min_estimate = min(.estimate),
    max_estimate = max(.estimate),
    .by = c(Sequence, Genes)
  ) %>%
  mutate(
    label = paste0(
      "Sequence: ",
      Sequence,
      ", Workflow: ",
      wflow,
      ", Measure: ",
      .metric)
  ) %>% 
  inner_join(gene_seq_names, by = "Genes")

p_gene_seq <-
  ggplot(dat_gene_seq, aes(text = Name)) +
  geom_pointrange(aes(
    x = Genes,
    y = mean_estimate,
    ymin = min_estimate,
    ymax = max_estimate
  )) +
  facet_wrap(~ label, ncol = 1) +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for Sequential Classifier")
ggplotly(p_gene_seq)

n_genes_seq <- 7
genes_add_seq <- gene_seq_names %>%
  filter(Genes > 0L) %>% 
  slice_min(order_by = Genes, n = n_genes_seq) %>% 
  pull(Name)
opt_genes_seq <- c(overlap_all, genes_add_seq)
```

In the sequential algorithm, sequences 1, 2, and 4 have relatively flat average F1-scores across the number of genes added. However, we can observe in sequence 3, the F1-score stabilizes at around 0.88 when we reach `r n_genes_seq` genes added, hence the optimal number of genes used will be n=`r length(overlap_all)`+`r n_genes_seq`=`r length(opt_genes_seq)` The added genes are: `r str_flatten_comma(genes_add_seq, last = " and ")`.

### SMOTE-Random Forest

```{r gene-smote-rf, fig.height=8, fig.cap='Gene Optimization for SMOTE-Random Forest Classifier'}
gene_smote_rf_names <- all_vi_train %>% 
  filter(wflow == "smote_rf") %>% 
  rowid_to_column("Genes") %>% 
  select(Genes, Name = Variable) %>% 
  add_row(Genes = 0L, Name = "Base", .before = 1)

dat_gene_smote_rf <- gene_opt_all_metrics_conf %>% 
  filter(.metric == "f_meas", class_group == "Overall") %>% 
  mutate(
    min_estimate = min(.estimate),
    max_estimate = max(.estimate),
    .by = Genes
  ) %>%
  mutate(label = paste0("Workflow: smote_rf, Measure: ", .metric)) %>% 
  inner_join(gene_smote_rf_names, by = "Genes")

p_gene_smote_rf <- 
  ggplot(dat_gene_smote_rf, aes(text = Name)) +
  geom_pointrange(aes(
    x = Genes,
    y = mean_estimate,
    ymin = min_estimate,
    ymax = max_estimate
  )) +
  facet_wrap(~ label, ncol = 1) +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for SMOTE-Random Forest Classifier")
ggplotly(p_gene_smote_rf)

n_genes_smote_rf <- 18
genes_add_smote_rf <- gene_smote_rf_names %>% 
  filter(Genes > 0L) %>% 
  slice_min(order_by = Genes, n = n_genes_smote_rf) %>% 
  pull(Name)
opt_genes_smote_rf <- c(overlap_all, genes_add_smote_rf)
```

In the SMOTE-Random Forest classifier, the F1-score stabilizes at around 0.7 when we reach `r n_genes_smote_rf` genes added, hence the optimal number of genes used will be n=`r length(overlap_all)`+`r n_genes_smote_rf`=`r length(opt_genes_smote_rf)` The added genes are: `r str_flatten_comma(genes_add_smote_rf, last = " and ")`.

## Test Set Performance

Now we'd like to see how our best methods perform in the confirmation and validation sets. The class-specific F1-scores will be used.

The top 2 methods are the sequential and SMOTE-Random Forest classifiers. We can test 2 additional methods by using either the full set of genes or the optimal set of genes for both of these classifiers.

```{r top-models}
# sequential data
seq_data <- readRDS(here("data", "seq_data.rds"))
seq_class <- readRDS(here("data", "seq_class.rds"))
seq_wflows <- readRDS(here("data", "seq_wflows.rds"))

# Sequential full model
set.seed(2024)
mod_seq_full <- list(all_models_seq,
                     seq_data,
                     seq_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>% fit(data = data)
  })

# Sequential optimal gene list model
set.seed(2024)
mod_seq_opt <- list(all_models_seq,
                    seq_data,
                    seq_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>%
      update_recipe(pluck(., "pre", "actions", "recipe", "recipe") %>%
                      step_select(c(all_of(!!opt_genes_seq), class),
                                  skip = TRUE)) %>%
      fit(data = data)
  })

# smote_rf full model
set.seed(2024)
train_ref <- cbind(train_data, class = train_class)
mod_smote_rf_full <- smote_rf_model %>% fit(data = train_ref)

# smote_rf optimal gene list model
set.seed(2024)
mod_smote_rf_opt <- smote_rf_model %>% 
  update_recipe(pluck(., "pre", "actions", "recipe", "recipe") %>%
                  step_select(c(all_of(!!opt_genes_smote_rf), class),
                              skip = TRUE)) %>%
  fit(data = train_ref)

# Per-class metric set
per_class_mset <-
  metric_set(accuracy, sensitivity, specificity, f_meas, bal_accuracy, kap)
```

### Confirmation Set

```{r confirmation-set}
# Load data
conf_class <- readRDS(here("data", "conf_class.rds"))
conf_data <- readRDS(here("data", "conf_data.rds"))
```

```{r conf-eval-overall}
# Confirmation set overall predictions
conf_pred_overall <- data.frame(FileName = rownames(conf_data),
                                Truth = conf_class) %>%
  mutate(
    c(p_seq_full = mod_seq_full,
      p_seq_opt = mod_seq_opt) %>%
      map(~ predict(., conf_data)[[".pred_class"]]) %>%
      bind_cols()
  ) %>%
  rename_with(~ gsub("(.*)\\..*_(.*)", "\\1_\\2", .), where(is.factor)) %>% 
  mutate(
    across(matches("p"), as.character),
    Prediction_sequential_full = case_when(
      !grepl("non", p_seq_full_s1) ~ p_seq_full_s1,
      !grepl("non", p_seq_full_s2) ~ p_seq_full_s2,
      !grepl("non", p_seq_full_s3) ~ p_seq_full_s3,
      .default = p_seq_full_s4
    ),
    Prediction_sequential_optimal = case_when(
      !grepl("non", p_seq_opt_s1) ~ p_seq_opt_s1,
      !grepl("non", p_seq_opt_s2) ~ p_seq_opt_s2,
      !grepl("non", p_seq_opt_s3) ~ p_seq_opt_s3,
      .default = p_seq_opt_s4
    ),
    Prediction_smote_rf_full = predict(mod_smote_rf_full, conf_data)[[".pred_class"]],
    Prediction_smote_rf_opt = predict(mod_smote_rf_opt, conf_data)[[".pred_class"]],
    across(c("Truth", matches("Prediction")), factor)
  ) %>%
  select(-starts_with("p", ignore.case = FALSE)) %>%
  pivot_longer(
    cols = matches("Prediction"),
    names_to = "Method",
    names_prefix = "Prediction_",
    values_to = "Prediction"
  ) %>% 
  mutate(across(c(Truth, Prediction),
                ~ factor(.x, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC")))) |> 
  mutate(Method = factor(
    case_match(
      Method,
      "sequential_full" ~ "Sequential, Full Set",
      "sequential_optimal" ~ "Sequential, Optimal Set",
      "smote_rf_full" ~ "SMOTE-Random Forest, Full Set",
      "smote_rf_opt" ~ "SMOTE-Random Forest, Optimal Set"
    )
  ))

# Confirmation set overall metrics
conf_eval_overall <- conf_pred_overall %>%
  group_by(Method) %>%
  summarize(
    Accuracy = accuracy_vec(Truth, Prediction),
    Sensitivity = sensitivity_vec(Truth, Prediction),
    Specificity = specificity_vec(Truth, Prediction),
    `F1-Score` = f_meas_vec(Truth, Prediction),
    `Balanced Accuracy` = bal_accuracy_vec(Truth, Prediction),
    Kappa = kap_vec(Truth, Prediction)
  )

kable(conf_eval_overall,
      caption = "Overall Evaluation Metrics on Confirmation Set Models",
      digits = 3) %>%
  kable_styling()
```

```{r conf-conf-mat, fig.cap='Confusion Matrices for Confirmation Set Models'}
conf_conf_mat <- conf_pred_overall %>% 
  nest(.by = Method) %>% 
  deframe() %>%
  imap(~ conf_mat(.x, Truth, Prediction) %>%
         autoplot(type = "heatmap") +
         labs(title = .y)) %>% 
  wrap_plots() +
  plot_annotation(
    title = "Confusion Matrices for Confirmation Set Models",
    tag_levels = "A"
  )

print(conf_conf_mat)
```

```{r conf-eval-per-class}
conf_eval_per_class <- conf_pred_overall %>%
  nest(.by = Method) %>%
  mutate(data = map(data, ~ ova_metrics(.x, Truth, Prediction, per_class_mset))) %>%
  unnest(data) %>%
  mutate(
    Metric = case_match(
      .metric,
      "accuracy" ~ "Accuracy",
      "sensitivity" ~ "Sensitivity",
      "specificity" ~ "Specificity",
      "f_meas" ~ "F1-Score",
      "bal_accuracy" ~ "Balanced Accuracy",
      "kap" ~ "Kappa"
    ),
    .keep = "unused"
  ) %>%
  pivot_wider(
    id_cols = c(Method, Metric),
    names_from = class_group,
    values_from = .estimate
  )

kable(conf_eval_per_class,
      caption = "Per-Class Evaluation Metrics on Confirmation Set Model",
      digits = 3) %>% 
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Histotypes" = 5))
```

```{r roc-curve-seq-full, fig.cap='ROC Curves for Sequential Full Model in Confirmation Set', fig.width=8, out.width='100%'}
conf_data_seq <- list(conf_data,
                      conf_data[conf_class != "HGSC",],
                      conf_data[!conf_class %in% c("HGSC", "CCOC"),],
                      conf_data[!conf_class %in% c("HGSC", "CCOC", "ENOC"),])

conf_class_seq <- list(
  fct_other(conf_class, keep = "HGSC", other_level = "non-HGSC"),
  fct_other(conf_class[conf_class != "HGSC"],  keep = "CCOC", other_level = "non-CCOC"),
  fct_other(conf_class[!conf_class %in% c("HGSC", "CCOC")],  keep = "ENOC", other_level = "non-ENOC"),
  factor(conf_class[!conf_class %in% c("HGSC", "CCOC", "ENOC")])
)

roc_curve_seq_full <- mod_seq_full %>% 
  set_names(map_chr(., ~ paste(pluck(.x, "fit", "fit", "lvl"), collapse = " vs. "))) %>% 
  list(conf_data_seq, conf_class_seq) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")%>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, matches(".pred")[1])
    auc <- preds %>% 
      roc_auc(truth, matches(".pred")[1]) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      )
  }) %>% 
  imap(~ .x + labs(title = .y)) %>% 
  wrap_plots(widths = c(1, 1)) +
  plot_annotation(
    title = "ROC Curves for Sequential, Full Set Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_seq_full)
```

```{r roc-curve-seq-opt, fig.cap='ROC Curves for Sequential Optimal Model in Confirmation Set', fig.width=8, out.width='100%'}
roc_curve_seq_opt <- mod_seq_opt %>% 
  set_names(map_chr(., ~ paste(pluck(.x, "fit", "fit", "lvl"), collapse = " vs. "))) %>% 
  list(conf_data_seq, conf_class_seq) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")%>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, matches(".pred")[1])
    auc <- preds %>% 
      roc_auc(truth, matches(".pred")[1]) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      )
  }) %>% 
  imap(~ .x + labs(title = .y)) %>% 
  wrap_plots(widths = c(1, 1)) +
  plot_annotation(
    title = "ROC Curves for Sequential, Optimal Set Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_seq_opt)
```

```{r roc-curve-smote-rf-full, fig.cap='ROC Curves for SMOTE-Random Forest, Full Set Model in Confirmation Set', fig.width=8, out.width='100%'}
preds <- predict(mod_smote_rf_full, conf_data, type = "prob") %>% 
  add_column(truth = factor(conf_class))
roc_df <- preds %>%
  roc_curve(truth, matches(".pred"))
auc <- preds %>%
  roc_auc(truth, matches(".pred")) %>% 
  pull(.estimate) %>% 
  number(accuracy = 0.001, prefix = "AUC = ")
roc_curve_smote_rf_full <-
  autoplot(roc_df) +
  labs(title = "ROC Curve for SMOTE-Random Forest, Full Set Model in Confirmation Set",
       subtitle = auc)
print(roc_curve_smote_rf_full)
```

```{r roc-curve-smote-rf-opt, fig.cap='ROC Curves for SMOTE-Random Forest, Optimal Set Model in Confirmation Set', fig.width=8, out.width='100%'}
preds <- predict(mod_smote_rf_opt, conf_data, type = "prob") %>% 
  add_column(truth = factor(conf_class))
roc_df <- preds %>%
  roc_curve(truth, matches(".pred"))
auc <- preds %>%
  roc_auc(truth, matches(".pred")) %>% 
  pull(.estimate) %>% 
  number(accuracy = 0.001, prefix = "AUC = ")
roc_curve_smote_rf_opt <- 
  autoplot(roc_df) +
  labs(title = "ROC Curve for SMOTE-Random Forest, Optimal Set Model in Confirmation Set",
       subtitle = auc)
print(roc_curve_smote_rf_opt)
```

### Validation Set

```{r validation-set}
# Load data
val_class <- readRDS(here("data", "val_class.rds"))
val_data <- readRDS(here("data", "val_data.rds"))
```

```{r val-eval-overall}
# Validation set overall predictions
val_pred_overall <- data.frame(FileName = rownames(val_data),
                               Truth = val_class) %>%
  mutate(
    Prediction_smote_rf_opt = predict(mod_smote_rf_opt, val_data)[[".pred_class"]],
    across(c("Truth", matches("Prediction")), factor)
  ) %>% 
  pivot_longer(
    cols = matches("Prediction"),
    names_to = "Method",
    names_prefix = "Prediction_",
    values_to = "Prediction"
  ) %>% 
  mutate(across(c(Truth, Prediction),
                ~ factor(.x, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))),
         Method = "SMOTE-Random Forest, Optimal Set")

# Validation set overall metrics
val_eval_overall <- val_pred_overall %>%
  group_by(Method) %>%
  summarize(
    Accuracy = accuracy_vec(Truth, Prediction),
    Sensitivity = sensitivity_vec(Truth, Prediction),
    Specificity = specificity_vec(Truth, Prediction),
    `F1-Score` = f_meas_vec(Truth, Prediction),
    `Balanced Accuracy` = bal_accuracy_vec(Truth, Prediction),
    Kappa = kap_vec(Truth, Prediction)
  )

kable(val_eval_overall,
      caption = "Overall Evaluation Metrics on Validation Set Model",
      digits = 3) %>% 
  kable_styling()
```

```{r val-conf-mat, fig.cap='Confusion Matrix for Validation Set Model'}
val_conf_mat <- val_pred_overall %>%
  nest(.by = Method) %>%
  deframe() %>%
  imap(~ conf_mat(.x, Truth, Prediction) %>%
         autoplot(type = "heatmap") +
         labs(title = .y)) %>% 
  wrap_plots() +
  plot_annotation(title = "Confusion Matrix for Validation Set Model")

print(val_conf_mat)
```

```{r val-eval-per-class}
val_eval_per_class <- val_pred_overall %>%
  nest(.by = Method) %>%
  mutate(data = map(data, ~ ova_metrics(.x, Truth, Prediction, per_class_mset))) %>%
  unnest(data) %>%
  mutate(
    Metric = case_match(
      .metric,
      "accuracy" ~ "Accuracy",
      "sensitivity" ~ "Sensitivity",
      "specificity" ~ "Specificity",
      "f_meas" ~ "F1-Score",
      "bal_accuracy" ~ "Balanced Accuracy",
      "kap" ~ "Kappa"
    ),
    .keep = "unused"
  ) |>
  pivot_wider(
    id_cols = c(Method, Metric),
    names_from = class_group,
    values_from = .estimate
  )

kable(val_eval_per_class,
      caption = "Per-Class Eevaluation Metrics on Validation Set Model",
      digits = 3) %>% 
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Histotypes" = 5))
```
