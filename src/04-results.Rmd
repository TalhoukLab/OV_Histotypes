# Results

```{r setup-04}
# Load packages and data
library(dplyr)
library(purrr)
library(tibble)
library(tidyr)
library(ggplot2)
library(forcats)
library(knitr)
library(kableExtra)
library(RankAggreg)
library(splendid)
library(magrittr)
library(DT)
library(plotly)
library(patchwork)
library(tidymodels)
library(themis)
library(here)
source(here("validation/cs_process_cohorts.R"))
source(here("src/funs.R"))

# Training data
train_data <- readRDS(here("data/train_data.rds"))
train_class <- readRDS(here("data/train_class.rds"))

# Metrics
all_metrics_train <- readRDS(here("data/all_metrics_train.rds"))
per_class_metrics_seq <-
  readRDS(here("data/per_class_metrics_seq.rds"))
per_class_metrics_two_step <-
  readRDS(here("data/per_class_metrics_two_step.rds"))

# Models
all_models_seq <- readRDS(here("data/all_models_seq.rds"))
all_models_two_step <- readRDS(here("data/all_models_two_step.rds"))

# Overall Variable Importance
all_vi_seq <- readRDS(here("data/all_vi_seq.rds"))
all_vi_two_step <- readRDS(here("data/all_vi_two_step.rds"))

# Gene Optimization
gene_opt_all_metrics_seq <-
  readRDS(here("data/gene_opt_all_metrics_seq.rds"))
gene_opt_all_metrics_two_step <- 
  readRDS(here("data/gene_opt_all_metrics_two_step.rds"))

# PrOTYPE and SPOT genes
coefmat <- readRDS(here("data/coefmat.rds"))
overlap_PrOTYPE <- intersect(names(train_data), cs5$Name)
overlap_SPOT <- intersect(names(train_data), coefmat[["Symbol"]])
overlap_all <- unique(c(overlap_PrOTYPE, overlap_SPOT))
```

We summarize cross-validated training performance of class metrics in the training set. The accuracy, F1-score, and kappa, are the metrics of interest. Workflows are ordered by their mean estimates across the outer folds of the nested CV for each metric.

## Training Set

```{r train-metrics}
all_metrics_train <- all_metrics_train %>%
  separate(wflow, c("Subsampling", "Algorithms"), remove = FALSE) %>%
  mutate(
    Subsampling = factor(Subsampling, levels = c("none", "down", "up", "smote", "hybrid")),
    Algorithms = factor(Algorithms, levels = c("rf", "svm", "xgb", "mr")),
    class_group = factor(
      class_group,
      levels = c("Overall", "HGSC", "CCOC", "ENOC", "LGSC", "MUC")
    )
  ) %>% 
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(wflow, class_group, .metric)
  ) %>% 
  mutate(
    undefined = case_when(any(is.nan(mean_estimate)) ~ "all",
                          any(is.na(.estimate)) ~ "some",
                          .default = "none"),
    .by = c(wflow, .metric)
  )
```

### Accuracy

```{r train-accuracy, fig.cap='Training Set Mean Accuracy'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Accuracy",
       title = "Training Set Mean Accuracy")
print(p)
```

```{r train-accuracy-table}
overall_accuracy_train <- summarize_metrics(all_metrics_train,
                                            metric = "accuracy",
                                            per_class = FALSE)

kable(overall_accuracy_train,
      caption = "Training Set Mean Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-accuracy-class, fig.cap='Training Set Class-Specific Mean Accuracy', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Accuracy",
       title = "Training Set Class-Specific Mean Accuracy")
print(p)
```

```{r train-accuracy-class-table}
class_accuracy_train <- summarize_metrics(all_metrics_train,
                                          metric = "accuracy",
                                          per_class = TRUE)
kable(class_accuracy_train,
      caption = "Training Set Class-Specific Mean Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Sensitivity

```{r train-sens, fig.cap='Training Set Mean Sensitivity'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "sensitivity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Sensitivity",
       title = "Training Set Mean Sensitivity")
print(p)
```

```{r train-sens-table}
overall_sens_train <- summarize_metrics(all_metrics_train,
                                        metric = "sensitivity",
                                        per_class = FALSE)
kable(overall_sens_train,
      caption = "Training Set Mean Sensitivity",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-sens-class, fig.cap='Training Set Class-Specific Mean Sensitivity', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "sensitivity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Sensitivity",
       title = "Training Set Class-Specific Mean Sensitivity")
print(p)
```

```{r train-sens-class-table}
class_sens_train <- summarize_metrics(all_metrics_train,
                                      metric = "sensitivity",
                                      per_class = TRUE)
kable(class_sens_train,
      caption = "Cross-Validated Training Set Class-Specific Mean Sensitivity",
      escape = FALSE) %>%
  kable_styling() %>%
  collapse_rows(columns = 1) %>%
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Specificity

```{r train-spec, fig.cap='Training Set Mean Specificity'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "specificity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Specificity",
       title = "Training Set Mean Specificity")
print(p)
```

```{r train-spec-table}
overall_spec_train <- summarize_metrics(all_metrics_train,
                                        metric = "specificity",
                                        per_class = FALSE)
kable(overall_spec_train,
      caption = "Training Set Mean Specificity",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-spec-class, fig.cap='Training Set Class-Specific Mean Specificity', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "specificity"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Specificity",
       title = "Training Set Class-Specific Mean Specificity")
print(p)
```

```{r train-spec-class-table}
class_spec_train <- summarize_metrics(all_metrics_train,
                                      metric = "specificity",
                                      per_class = TRUE)
kable(class_spec_train,
      caption = "Cross-Validated Training Set Class-Specific Mean Specificity",
      escape = FALSE) %>%
  kable_styling() %>%
  collapse_rows(columns = 1) %>%
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### F1-Score

```{r train-f1, fig.cap='Training Set Mean F1-Score'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "f_meas"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "F1-Score",
       title = "Training Set Mean F1-Score")
print(p)
```

```{r train-f1-table}
overall_f1_train <- summarize_metrics(all_metrics_train,
                                      metric = "f_meas",
                                      per_class = FALSE)
kable(overall_f1_train,
      caption = "Training Set Mean F1-Score",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-f1-class, fig.cap='Training Set Class-Specific Mean F1-Score', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "f_meas"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "F1-Score",
       title = "Training Set Class-Specific Mean F1-Score")
print(p)
```

```{r train-f1-class-table}
class_f1_train <- summarize_metrics(all_metrics_train,
                                    metric = "f_meas",
                                    per_class = TRUE)
kable(class_f1_train,
      caption = "Cross-Validated Training Set Class-Specific Mean F1-Score",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Balanced Accuracy

```{r train-bal-accuracy, fig.cap='Training Set Mean Balanced Accuracy'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "bal_accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Balanced Accuracy",
       title = "Training Set Mean Balanced Accuracy")
print(p)
```

```{r train-bal-accuracy-table}
overall_bal_accuracy_train <- summarize_metrics(all_metrics_train,
                                                metric = "bal_accuracy",
                                                per_class = FALSE)

kable(overall_bal_accuracy_train,
      caption = "Training Set Mean Balanced Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-bal-accuracy-class, fig.cap='Training Set Class-Specific Mean Balanced Accuracy', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "bal_accuracy"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Balanced Accuracy",
       title = "Training Set Class-Specific Mean Balanced Accuracy")
print(p)
```

```{r train-bal-accuracy-class-table}
class_bal_accuracy_train <- summarize_metrics(all_metrics_train,
                                              metric = "bal_accuracy",
                                              per_class = TRUE)
kable(class_bal_accuracy_train,
      caption = "Training Set Class-Specific Mean Balanced Accuracy",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

### Kappa

```{r train-kappa, fig.cap='Training Set Mean Kappa'}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group == "Overall", .metric == "kap"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Mean Kappa")
print(p)
```

```{r train-kappa-table}
overall_kappa_train <- summarize_metrics(all_metrics_train,
                                         metric = "kap",
                                         per_class = FALSE)
kable(overall_kappa_train,
      caption = "Training Set Mean Kappa",
      escape = FALSE) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Algorithms" = 4))
```

```{r train-kappa-class, fig.cap='Training Set Class-Specific Mean Kappa', fig.height=8}
p <- ggplot(
  all_metrics_train %>%
    filter(class_group != "Overall", .metric == "kap"),
  aes(
    x = fct_reorder(wflow, mean_estimate, .desc = TRUE, .na_rm = FALSE),
    y = mean_estimate,
    ymin = lower,
    ymax = upper,
    color = Algorithms,
    shape = Subsampling
  )
) +
  geom_pointrange() +
  facet_wrap(~ class_group, ncol = 1) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Workflow",
       y = "Kappa",
       title = "Training Set Class-Specific Mean Kappa")
print(p)
```

```{r train-kappa-class-table}
class_kappa_train <- summarize_metrics(all_metrics_train,
                                       metric = "kap",
                                       per_class = TRUE)
kable(class_kappa_train,
      caption = "Training Set Class-Specific Mean Kappa",
      escape = FALSE) %>%
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Algorithms" = 4))
```

## Rank Aggregation

Multi-step methods:

- **sequential**: sequential algorithm with SMOTE subsampling at every step. The sequence of algorithms used are:
  - HGSC vs. non-HGSC using random forest
  - CCOC vs. non-CCOC using random forest
  - ENOC vs. non-ENOC using support vector machine
  - LGSC vs. MUC using random forest
- **two_step**: two-step algorithm sequence of subsampling methods and algorithms used are:
  - HGSC vs. non-HGSC using SMOTE subsampling and random forest
  - CCOC vs. ENOC vs. MUC vs. LGSC using hybrid subsampling and support vector machine

```{r per-class-metrics}
# Add lower, upper, undefined columns for per-class sequential, two_step
per_class_metrics_seq <- per_class_metrics_seq |>
  mutate(class_group = factor(class_group, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))) |>
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(class_group, .metric)
  ) |>
  mutate(undefined = case_when(any(is.nan(mean_estimate)) ~ "all", any(is.na(.estimate)) ~ "some", .default = "none"),
         .by = .metric) |>
  add_column(wflow = "sequential", .before = 1)

per_class_metrics_two_step <- per_class_metrics_two_step |>
  mutate(class_group = factor(class_group, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))) |>
  mutate(
    lower = ifelse(all(is.na(.estimate)), NA, min(.estimate, na.rm = TRUE)),
    upper = ifelse(all(is.na(.estimate)), NA, max(.estimate, na.rm = TRUE)),
    .by = c(class_group, .metric)
  ) |>
  mutate(undefined = case_when(any(is.nan(mean_estimate)) ~ "all", any(is.na(.estimate)) ~ "some", .default = "none"),
         .by = .metric) |>
  add_column(wflow = "two_step", .before = 1)

# Combine all per-class metrics
per_class_metrics <- bind_rows(all_metrics_train,
                                per_class_metrics_seq,
                                per_class_metrics_two_step) |>
  filter(class_group != "Overall") |>
  droplevels()

# Keep only workflows that predicted at least one case in each class
per_class_f1 <- per_class_metrics %>%
  filter(.metric == "f_meas") %>%
  distinct(pick(-fold_id, -.estimate)) %>% 
  select(wflow, class_group, mean_estimate) %>%
  arrange(class_group) %>% 
  pivot_wider(names_from = "class_group", values_from = "mean_estimate") %>%
  drop_na() %>%
  pivot_longer(-wflow, names_to = "class_group", values_to = "mean_estimate")

# Rank F1-measures
wflow_ranks <- per_class_f1 %>%
  group_by(class_group) %>%
  mutate(rank = factor(row_number(-mean_estimate))) %>%
  ungroup() %>%
  arrange(rank) %>%
  pivot_wider(id_cols = class_group,
              names_from = "rank",
              values_from = "wflow") %>%
  column_to_rownames("class_group") %>%
  as.matrix()

# Rank aggregation using genetic algorithm
wflow_rank_agg <- splendid:::sink_output(RankAggreg(
  x = wflow_ranks,
  k = ncol(wflow_ranks),
  method = "GA",
  seed = 2022,
  verbose = FALSE
)) %>% 
  pluck("top.list") %>% 
  enframe(name = "rank", value = "wflow")

# F1-score summary
f1_summary <- per_class_f1 %>% 
  pivot_wider(names_from = class_group, values_from = mean_estimate) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 3))) %>% 
  inner_join(wflow_rank_agg, by = "wflow") %>% 
  arrange(rank) %>% 
  rename(Workflow = wflow, Rank = rank)

# Interactive table showing F1-scores used in rank aggregation
datatable(
  f1_summary,
  options = list(
    scrollX = TRUE,
    fixedColumns = TRUE,
    columnDefs = list(
      list(
        targets = 1,
        render = JS("$.fn.dataTable.render.ellipsis( 10 )")
      ),
      list(type = "natural", targets = 0)
    ),
    pageLength = 50
  ), 
  rownames = FALSE,
  caption = "F1-Score Summary by Workflow and Class",
  filter = "top",
  extensions = "FixedColumns",
  plugins = c("ellipsis", "natural")
)

# Look at Top 5 workflows
wflow_top <- head(wflow_rank_agg, 5)
```

The `r ncol(wflow_ranks)` workflows are ordered in the table by their aggregated ranks using the Genetic Algorithm. We see that the best performing methods involve the sequential and two-step algorithms.

### Top Workflows

We look at the per-class evaluation metrics of the top `r nrow(wflow_top)` workflows.

```{r metrics-top-by-class, fig.cap=paste('Top', nrow(wflow_top), 'Workflow Per-Class Evaluation Metrics by Class')}
metrics_top <- per_class_metrics %>%
  distinct(pick(-fold_id, -.estimate)) %>%
  inner_join(wflow_top, by = "wflow") %>%
  mutate(
    wflow = fct_reorder(wflow, rank),
    .metric = case_match(
      .metric,
      "accuracy" ~ "Accuracy",
      "sensitivity" ~ "Sensitivity",
      "specificity" ~ "Specificity",
      "f_meas" ~ "F1-Score",
      "bal_accuracy" ~ "Balanced Accuracy",
      "kap" ~ "Kappa",
      .ptype = factor(
        levels = c(
          "Accuracy",
          "Sensitivity",
          "Specificity",
          "F1-Score",
          "Balanced Accuracy",
          "Kappa"
        )
      )
    )
  ) |> 
  drop_na(.metric) |> 
  arrange(rank)
  
p <-
  ggplot(metrics_top) +
  geom_pointrange(
    aes(
      x = wflow,
      y = mean_estimate,
      ymin = lower,
      ymax = upper,
      color = .metric
    ),
    position = position_jitter(width = 0.3),
    fatten = 1
  ) +
  facet_wrap(vars(class_group), ncol = 2) +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  ) +
  labs(
    x = "Workflow",
    y = "Metric Value",
    color = "Metric",
    title = paste("Top", nrow(wflow_top), "Workflow Per-Class Evaluation Metrics by Class")
  )
print(p)
```

```{r metrics-top-by-metric, fig.cap=paste('Top', nrow(wflow_top), 'Workflow Per-Class Evaluation Metrics by Metric')}
p <-
  ggplot(metrics_top) +
  geom_pointrange(
    aes(
      x = wflow,
      y = mean_estimate,
      ymin = lower,
      ymax = upper,
      color = class_group
    ),
    position = position_jitter(width = 0.3),
    fatten = 1
  ) +
  facet_wrap(vars(.metric), ncol = 2, scales = "free_y") +
  theme_bw() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  ) +
  labs(
    x = "Workflow",
    y = "Metric Value",
    color = "Class",
    title = paste("Top", nrow(wflow_top), "Workflow Per-Class Evaluation Metrics by Metric")
  )
print(p)
```

Misclassified cases from a previous step of the sequence of classifiers are not included in subsequent steps of the training set CV folds. Thus, we cannot piece together the test set predictions from the sequential and two-step algorithms to obtain overall metrics.

## Optimal Gene Sets

### Sequential Algorithm

```{r gene-seq, fig.height=8, fig.cap='Gene Optimization for Sequential Classifier'}
gene_seq_names <- all_vi_seq %>% 
  rename(Name = Variable) %>% 
  rowid_to_column("Genes") %>% 
  add_row(Genes = 0L, Name = "Base", .before = 1)

dat_gene_seq <- gene_opt_all_metrics_seq %>% 
  filter(.metric == "f_meas", class_group == "Overall") %>% 
  mutate(
    min_estimate = min(.estimate),
    max_estimate = max(.estimate),
    .by = c(Sequence, Genes)
  ) %>%
  mutate(
    label = paste0(
      "Sequence: ",
      Sequence,
      ", Workflow: ",
      wflow,
      ", Measure: ",
      .metric)
  ) %>% 
  inner_join(gene_seq_names, by = "Genes")

p_gene_seq <-
  ggplot(dat_gene_seq, aes(text = Name)) +
  geom_pointrange(aes(
    x = Genes,
    y = mean_estimate,
    ymin = min_estimate,
    ymax = max_estimate
  )) +
  facet_wrap(~ label, ncol = 1) +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for Sequential Classifier")
ggplotly(p_gene_seq)

n_genes_seq <- 11
genes_add_seq <- gene_seq_names %>%
  filter(Genes > 0L) %>% 
  slice_min(order_by = Genes, n = n_genes_seq) %>% 
  pull(Name)
opt_genes_seq <- c(overlap_all, genes_add_seq)
```

In the sequential algorithm, sequences 1, 2, and 4 have relatively flat average F1-scores across the number of genes added. However, we can observe in sequence 3, the F1-score stabilizes at around 0.91 when we reach `r n_genes_seq` genes added, hence the optimal number of genes used will be n=`r length(overlap_all)`+`r n_genes_seq`=`r length(opt_genes_seq)` The added genes are: `r str_flatten_comma(genes_add_seq, last = " and ")`.

### Two-Step Algorithm

```{r gene-2s, fig.cap='Gene Optimization for Two-Step Classifier'}
gene_2s_names <- all_vi_two_step %>% 
  rename(Name = Variable) %>% 
  rowid_to_column("Genes") %>% 
  add_row(Genes = 0L, Name = "Base", .before = 1)

dat_gene_2s <- gene_opt_all_metrics_two_step %>% 
  filter(.metric == "f_meas", class_group == "Overall") %>% 
  mutate(
    min_estimate = min(.estimate),
    max_estimate = max(.estimate),
    .by = c(Sequence, Genes)
  ) %>%
  mutate(
    label = paste0(
      "Sequence: ",
      Sequence,
      ", Workflow: ",
      wflow,
      ", Measure: ",
      .metric)
  ) %>% 
  inner_join(gene_2s_names, by = "Genes")

p_gene_2s <-
  ggplot(dat_gene_2s, aes(text = Name)) +
  geom_pointrange(aes(
    x = Genes,
    y = mean_estimate,
    ymin = min_estimate,
    ymax = max_estimate
  )) +
  facet_wrap(~ label, ncol = 1) +
  theme_bw() +
  labs(x = "Genes Added",
       y = "F1-Score",
       title = "Gene Optimziation for Two-Step Classifier")
ggplotly(p_gene_2s)

n_genes_2s <- 12
genes_add_2s <- gene_2s_names %>%
  filter(Genes > 0L) %>% 
  slice_min(order_by = Genes, n = n_genes_2s) %>% 
  pull(Name)
opt_genes_2s <- c(overlap_all, genes_add_2s)
```

The second step of the classifier fits a multinomial model, so we can use the macro F1-score as the metric to analyze gene entry. In the two-step classifier, we see that in Step 2, the macro F1-score stabilizes at around 0.80 when we reach `r n_genes_2s` added.

```{r gene-2s-per-class, fig.height=8, fig.cap='Gene Optimization for Step 2 of Two-Step Classifier'}
f1_threshold <- 0.75

dat_gene_2s_per_class <- gene_opt_all_metrics_two_step %>%
  filter(.metric == "f_meas", Sequence == 2, class_group != "Overall") %>% 
  mutate(
    min_estimate = min(.estimate),
    max_estimate = max(.estimate),
    .by = c(class_group, Genes)
  ) %>%
  mutate(
    label = paste0(
      "Sequence: ",
      Sequence,
      ", Workflow: ",
      wflow,
      ", Measure: ",
      .metric)
  ) %>% 
  inner_join(gene_2s_names, by = "Genes") %>%
  group_by(class_group) %>%
  mutate(min_genes = ifelse(Genes == min(Genes[mean_estimate >= f1_threshold]), "Yes", "No")) %>%
  ungroup()
  
p_gene_2s_per_class <- ggplot(dat_gene_2s_per_class, aes(text = Name)) +
  geom_pointrange(aes(
    x = Genes,
    y = mean_estimate,
    ymin = min_estimate,
    ymax = max_estimate,
    color = min_genes
  )) +
  scale_color_brewer(palette = "Dark2") +
  facet_wrap(~ class_group, ncol = 1, scales = "free") +
  theme_bw() +
  labs(
    x = "Genes Added",
    y = "Per-Class F1-Score",
    title = "Gene Optimziation for Step 2 of Two-Step Classifier",
    subtitle = "Threshold: Fewest genes required for mean per-class F1-score >= 0.75",
    color = "Minimum Genes"
  )

ggplotly(p_gene_2s_per_class)

n_genes_2s <- 10
genes_add_2s <- gene_2s_names %>%
  filter(Genes > 0L) %>% 
  slice_min(order_by = Genes, n = n_genes_2s) %>% 
  pull(Name)
opt_genes_2s <- c(overlap_all, genes_add_2s)
```

Alternatively, we can analyze the per-class F1-scores in the second step of the classifier to see whether all of the non-HGSC classes have relatively good performance at the same number of genes. The threshold condition that we consider is the fewest genes required such that the mean per-class F1 score is at least 0.75. We see that `r n_genes_2s` genes are required to reach that threshold for ENOC. Hence, the optimal number of genes used will be n=`r length(overlap_all)`+`r n_genes_2s`=`r length(opt_genes_2s)`. The added genes are: `r str_flatten_comma(genes_add_2s, last = " and ")`.

## Test Set Performance

Now we'd like to see how our best methods perform in the confirmation and validation sets. The class-specific F1-scores will be used.

The top 2 methods are the sequential and two-step algorithms. We can test 2 additional methods by using either the full set of genes or the optimal set of genes for both of these algorithms.

```{r top-models}
# Two-step data
two_step_data <- readRDS(here("data", "two_step_data.rds"))
two_step_class <- readRDS(here("data", "two_step_class.rds"))

# Two-step full model
set.seed(2024)
mod_two_step_full <- list(all_models_two_step,
                          two_step_data,
                          two_step_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>% fit(data = data)
  })

# Two-step optimal gene list model
set.seed(2024)
mod_two_step_opt <- list(all_models_two_step,
                         two_step_data,
                         two_step_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>%
      update_recipe(pluck(., "pre", "actions", "recipe", "recipe") %>%
                      step_select(c(all_of(!!opt_genes_2s), class),
                                  skip = TRUE)) %>%
      fit(data = data)
  })

# sequential data
seq_data <- readRDS(here("data", "seq_data.rds"))
seq_class <- readRDS(here("data", "seq_class.rds"))
seq_wflows <- readRDS(here("data", "seq_wflows.rds"))

# Sequential full model
set.seed(2024)
mod_seq_full <- list(all_models_seq,
                     seq_data,
                     seq_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>% fit(data = data)
  })

# Sequential optimal gene list model
set.seed(2024)
mod_seq_opt <- list(all_models_seq,
                    seq_data,
                    seq_class) %>% 
  pmap(~ {
    data = cbind(..2, class = ..3)
    ..1 %>%
      update_recipe(pluck(., "pre", "actions", "recipe", "recipe") %>%
                      step_select(c(all_of(!!opt_genes_seq), class),
                                  skip = TRUE)) %>%
      fit(data = data)
  })

# Per-class metric set
per_class_mset <-
  metric_set(accuracy, sensitivity, specificity, f_meas, bal_accuracy, kap)
```

### Confirmation Set

```{r confirmation-set}
# Load data
conf_class <- readRDS(here("data", "conf_class.rds"))
conf_data <- readRDS(here("data", "conf_data.rds"))
```

```{r conf-eval-overall}
# Confirmation set overall predictions
conf_pred_overall <- data.frame(FileName = rownames(conf_data),
                                Truth = conf_class) %>%
  mutate(
    c(
      p_2s_full = mod_two_step_full,
      p_2s_opt = mod_two_step_opt,
      p_seq_full = mod_seq_full,
      p_seq_opt = mod_seq_opt
    ) %>%
      map(~ predict(., conf_data)[[".pred_class"]]) %>%
      bind_cols()
  ) %>% 
  rename_with(~ gsub("(.*)\\..*_(.*)", "\\1_\\2", .), where(is.factor)) %>% 
  mutate(
    across(matches("p"), as.character),
    Prediction_two_step_full = ifelse(!grepl("non", p_2s_full_s1),
                                      p_2s_full_s1, p_2s_full_s2),
    Prediction_two_step_optimal = ifelse(!grepl("non", p_2s_opt_s1),
                                         p_2s_opt_s1, p_2s_opt_s2), 
    Prediction_sequential_full = case_when(
      !grepl("non", p_seq_full_s1) ~ p_seq_full_s1,
      !grepl("non", p_seq_full_s2) ~ p_seq_full_s2,
      !grepl("non", p_seq_full_s3) ~ p_seq_full_s3,
      .default = p_seq_full_s4
    ),
    Prediction_sequential_optimal = case_when(
      !grepl("non", p_seq_opt_s1) ~ p_seq_opt_s1,
      !grepl("non", p_seq_opt_s2) ~ p_seq_opt_s2,
      !grepl("non", p_seq_opt_s3) ~ p_seq_opt_s3,
      .default = p_seq_opt_s4
    ),
    across(c("Truth", matches("Prediction")), factor)
  ) %>%
  select(-starts_with("p", ignore.case = FALSE)) %>%
  pivot_longer(
    cols = matches("Prediction"),
    names_to = "Method",
    names_prefix = "Prediction_",
    values_to = "Prediction"
  ) %>% 
  mutate(across(c(Truth, Prediction),
         ~ factor(.x, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))))

# Confirmation set overall metrics
conf_eval_overall <- conf_pred_overall %>%
  group_by(Method) %>%
  summarize(
    accuracy = accuracy_vec(Truth, Prediction),
    sensitivity = sensitivity_vec(Truth, Prediction),
    specificity = specificity_vec(Truth, Prediction),
    f1 = f_meas_vec(Truth, Prediction),
    bal_accuracy = bal_accuracy_vec(Truth, Prediction),
    kappa = kap_vec(Truth, Prediction)
  )

kable(conf_eval_overall,
      caption = "Overall Evaluation Metrics on Confirmation Set Models",
      digits = 3) %>%
  kable_styling()
```

```{r conf-conf-mat, fig.cap='Confusion Matrices for Confirmation Set Models'}
conf_conf_mat <- conf_pred_overall %>% 
  nest(.by = Method) %>% 
  deframe() %>%
  imap(~ conf_mat(.x, Truth, Prediction) %>%
         autoplot(type = "heatmap") +
         labs(title = .y)) %>% 
  wrap_plots() +
  plot_annotation(
    title = "Confusion Matrices for Confirmation Set Models",
    tag_levels = "A"
  )

print(conf_conf_mat)
```

```{r conf-eval-per-class}
conf_eval_per_class <- conf_pred_overall %>%
  nest(.by = Method) %>%
  mutate(data = map(data, ~ ova_metrics(.x, Truth, Prediction,
                                        per_class_mset))) %>%
  unnest(data) %>%
  mutate(Metric = fct_recode(.metric, f1 = "f_meas", kappa = "kap"),
         .keep = "unused") %>%
  pivot_wider(
    id_cols = c(Method, Metric),
    names_from = class_group,
    values_from = .estimate
  )

kable(conf_eval_per_class,
      caption = "Per-Class Evaluation Metrics on Confirmation Set Model",
      digits = 3) %>% 
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Histotypes" = 5))
```

```{r roc-curve-seq-full, fig.cap='ROC Curves for Sequential Full Model in Confirmation Set', fig.width=8, out.width='100%'}
conf_data_seq <- list(conf_data,
                      conf_data[conf_class != "HGSC",],
                      conf_data[!conf_class %in% c("HGSC", "CCOC"),],
                      conf_data[!conf_class %in% c("HGSC", "CCOC", "ENOC"),])

conf_class_seq <- list(
  fct_other(conf_class, keep = "HGSC", other_level = "non-HGSC"),
  fct_other(conf_class[conf_class != "HGSC"],  keep = "CCOC", other_level = "non-CCOC"),
  fct_other(conf_class[!conf_class %in% c("HGSC", "CCOC")],  keep = "ENOC", other_level = "non-ENOC"),
  factor(conf_class[!conf_class %in% c("HGSC", "CCOC", "ENOC")])
)

roc_curve_seq_full <- mod_seq_full %>% 
  set_names(map_chr(., ~ paste(pluck(.x, "fit", "fit", "lvl"), collapse = " vs. "))) %>% 
  list(conf_data_seq, conf_class_seq) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")%>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, matches(".pred")[1])
    auc <- preds %>% 
      roc_auc(truth, matches(".pred")[1]) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      )
  }) %>% 
  imap(~ .x + labs(title = .y)) %>% 
  wrap_plots(widths = c(1, 1)) +
  plot_annotation(
    title = "ROC Curves for Sequential Full Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_seq_full)
```

```{r roc-curve-seq-opt, fig.cap='ROC Curves for Sequential Optimal Model in Confirmation Set', fig.width=8, out.width='100%'}
roc_curve_seq_opt <- mod_seq_opt %>% 
  set_names(map_chr(., ~ paste(pluck(.x, "fit", "fit", "lvl"), collapse = " vs. "))) %>% 
  list(conf_data_seq, conf_class_seq) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")%>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, matches(".pred")[1])
    auc <- preds %>% 
      roc_auc(truth, matches(".pred")[1]) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      )
  }) %>% 
  imap(~ .x + labs(title = .y)) %>% 
  wrap_plots(widths = c(1, 1)) +
  plot_annotation(
    title = "ROC Curves for Sequential Optimal Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_seq_opt)
```

```{r roc-curve-two-step-full, fig.cap='ROC Curves for Two-Step Full Model in Confirmation Set', fig.width=10, out.width='100%'}
conf_data_two_step <- list(conf_data,
                           conf_data[conf_class != "HGSC", ])

conf_class_two_step <-
  list(factor(ifelse(conf_class == "HGSC", "HGSC", "non-HGSC")),
       factor(conf_class[conf_class != "HGSC"]))

roc_curve_two_step_full <- mod_two_step_full %>% 
  set_names(map_chr(., ~ paste(pluck(.x, "fit", "fit", "lvl"), collapse = " vs. "))) %>% 
  list(conf_data_two_step, conf_class_two_step) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")
    prob_cols <- if (ncol(preds) == 2) names(preds)[1] else names(preds)
    preds <- preds %>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, all_of(prob_cols))
    auc <- preds %>% 
      roc_auc(truth, all_of(prob_cols)) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      )
  }) %>% 
  imap(~ .x + labs(title = .y)) %>% 
  wrap_plots(widths = c(1, 2)) +
  plot_annotation(
    title = "ROC Curves for Two-Step Full Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_two_step_full)
```

```{r roc-curve-two-step-opt, fig.cap='ROC Curves for Two-Step Optimal Model in Confirmation Set', fig.width=10, out.width='100%'}
roc_curve_two_step_opt <- mod_two_step_opt %>% 
  set_names(map_chr(., ~ paste(pluck(.x, "fit", "fit", "lvl"), collapse = " vs. "))) %>% 
  list(conf_data_two_step, conf_class_two_step) %>% 
  pmap(~ {
    preds <- predict(..1, ..2, type = "prob")
    prob_cols <- if (ncol(preds) == 2) names(preds)[1] else names(preds)
    preds <- preds %>% 
      add_column(truth = ..3)
    roc_df <- preds %>% 
      roc_curve(truth, all_of(prob_cols))
    auc <- preds %>% 
      roc_auc(truth, all_of(prob_cols)) %>% 
      pull(.estimate) %>% 
      number(accuracy = 0.001, prefix = "AUC = ")
    autoplot(roc_df) +
      geom_label(
        x = 1,
        y = 0,
        hjust = 1,
        vjust = 0,
        label = auc,
        size = 3
      )
  }) %>% 
  imap(~ .x + labs(title = .y)) %>% 
  wrap_plots(widths = c(1, 2)) +
  plot_annotation(
    title = "ROC Curves for Two-Step Optimal Model in Confirmation Set",
    tag_levels = "A"
  )
print(roc_curve_two_step_opt)
```

### Validation Set

```{r validation-set}
# Load data
val_class <- readRDS(here("data", "val_class.rds"))
val_data <- readRDS(here("data", "val_data.rds"))
```

```{r val-eval-overall}
# Validation set overall predictions
val_pred_overall <- data.frame(FileName = rownames(val_data),
                               Truth = val_class) %>%
  mutate(
    c(p_2s_opt = mod_two_step_opt) %>%
      map(~ predict(., val_data)[[".pred_class"]]) %>%
      bind_cols()
  ) %>% 
  rename_with(~ gsub("(.*)\\..*_(.*)", "\\1_\\2", .), where(is.factor)) %>% 
  
  mutate(
    across(matches("p"), as.character),
    Prediction_two_step_optimal = ifelse(!grepl("non", p_2s_opt_s1),
                                         p_2s_opt_s1, p_2s_opt_s2),
    across(c("Truth", matches("Prediction")), factor)
  ) %>%
  select(-starts_with("p", ignore.case = FALSE)) %>%
  pivot_longer(
    cols = matches("Prediction"),
    names_to = "Method",
    names_prefix = "Prediction_",
    values_to = "Prediction"
  ) %>% 
  mutate(across(c(Truth, Prediction),
         ~ factor(.x, levels = c("HGSC", "CCOC", "ENOC", "LGSC", "MUC"))))

# Validation set overall metrics
val_eval_overall <- val_pred_overall %>%
  group_by(Method) %>%
  summarize(
    accuracy = accuracy_vec(Truth, Prediction),
    sensitivity = sensitivity_vec(Truth, Prediction),
    specificity = specificity_vec(Truth, Prediction),
    f1 = f_meas_vec(Truth, Prediction),
    bal_accuracy = bal_accuracy_vec(Truth, Prediction),
    kappa = kap_vec(Truth, Prediction)
  )

kable(val_eval_overall,
      caption = "Overall Evaluation Metrics on Validation Set Model",
      digits = 3) %>% 
  kable_styling()
```

```{r val-conf-mat, fig.cap='Confusion Matrix for Validation Set Model'}
val_conf_mat <- val_pred_overall %>%
  nest(.by = Method) %>%
  deframe() %>%
  imap(~ conf_mat(.x, Truth, Prediction) %>%
         autoplot(type = "heatmap") +
         labs(title = .y)) %>% 
  wrap_plots() +
  plot_annotation(title = "Confusion Matrix for Validation Set Model")

print(val_conf_mat)
```

```{r val-eval-per-class}
val_eval_per_class <- val_pred_overall %>%
  nest(.by = Method) %>%
  mutate(data = map(data, ~ ova_metrics(.x, Truth, Prediction,
                                        per_class_mset))) %>%
  unnest(data) %>%
  mutate(Metric = fct_recode(.metric, f1 = "f_meas", kappa = "kap"),
         .keep = "unused") %>%
  pivot_wider(
    id_cols = c(Method, Metric),
    names_from = class_group,
    values_from = .estimate
  )

kable(val_eval_per_class,
      caption = "Per-Class Eevaluation Metrics on Validation Set Model",
      digits = 3) %>% 
  kable_styling() %>% 
  collapse_rows(columns = 1) %>% 
  add_header_above(c(" " = 2, "Histotypes" = 5))
```
