# Methods

```{r setup-02}
source(here::here("validation/cs_process.R"))
source(here::here("validation/cs1_generate.R"))
source(here::here("validation/cs2_generate.R"))
```

## Data Processing

Normalizing CS2 to CS3 can easily follow the [PrOType](https://dchiu911.shinyapps.io/PrOType/) method for HGSC subtypes because both CodeSets have pool samples. A different technique is implemented when normalizing CS1 to CS3 where we use common samples and genes as reference sets.

### Raw Data

There are 3 NanoString CodeSets:

* CS1: OvCa2103_C953
  * Samples = `r ncol(cs1) - 3`
  * Genes = `r nrow(cs1)`
* CS2: PrOTYPE2_v2_C1645
  * Samples = `r ncol(cs2) - 3`
  * Genes = `r nrow(cs2)`
* CS3: OTTA2014_C2822
  * Samples = `r ncol(cs3) - 3`
  * Genes = `r nrow(cs3)`
  
These datasets contain raw counts extracted straight from NanoString RCC files.

```{r setup-unique-venn-diagrams, include=FALSE}
library(VennDiagram)
futile.logger::flog.threshold(futile.logger::ERROR, name = "VennDiagramLogger")

cs1_id <- cs_common[which(cs_common$CS1 != 0 & !is.na(cs_common$CS1)),]
cs2_id <- cs_common[which(cs_common$CS2 != 0 & !is.na(cs_common$CS2)),]
cs3_id <- cs_common[which(cs_common$CS3 != 0 & !is.na(cs_common$CS3) & !is.na(cs_common$summaryID)),]

#venn diagram of overlap between CodeSet samples by summary ID
#does not include total count of duplicate samples
venn_id <- venn.diagram(
  x = list(cs1_id$summaryID, cs2_id$summaryID, cs3_id$summaryID),
  category.names = c("CodeSet1", "CodeSet2", "CodeSet3"),
  filename = NULL,
  output = TRUE,
  imagetype = "png",
  height = 400,
  width = 400,
  resolution = 300, 
  compression = "lzw",
  lwd = 1,
  col = c("#440154ff", '#21908dff', '#fde725ff'), 
  fill = c(alpha("#440154ff", 0.3),
           alpha('#21908dff', 0.3),
           alpha('#fde725ff', 0.3)),
  cex = 2,
  fontfamily = "sans",
  cat.cex = 1.5,
  cat.default.pos = "outer",
  cat.pos = c(-27, 27, 135),
  cat.dist = c(0.055, 0.055, 0.085),
  cat.fontfamily = "sans",
  rotation = 1
) 

venn_genes <- venn.diagram(
  x = list(cs1_norm$Name, cs2_norm$Name, cs3_norm$Name),
  category.names = c("CodeSet1", "CodeSet2", "CodeSet3"),
  filename = NULL,
  output = TRUE,
  imagetype = "png",
  height = 400, 
  width = 400, 
  resolution = 300,
  compression = "lzw",
  lwd = 1,
  col = c("#440154ff", '#21908dff', '#fde725ff'), 
  fill = c(alpha("#440154ff", 0.3),
           alpha('#21908dff', 0.3),
           alpha('#fde725ff', 0.3)), 
  cex = 2,
  fontfamily = "sans",
  cat.cex = 1.5,
  cat.default.pos = "outer",
  cat.pos = c(-27, 27, 135),
  cat.dist = c(0.055, 0.055, 0.085),
  cat.fontfamily = "sans",
  rotation = 1
) 
```

### Housekeeping Genes

The first normalization step is to normalize all endogenous genes to housekeeping genes (POLR1B, SDHA, PGK1, ACTB, RPL19; reference genes expressed in all cells). We normalize by subtracting the average log~2~ housekeeping gene expression from the log~2~ endogenous gene expression. The updated CodeSet dimensions are now:

* CS1: OvCa2103_C953
  * Samples = `r ncol(cs1_norm) - 3`
  * Genes = `r nrow(cs1_norm)`
* CS2: PrOTYPE2_v2_C1645
  * Samples = `r ncol(cs2_norm) - 3`
  * Genes = `r nrow(cs2_norm)`
* CS3: OTTA2014_C2822
  * Samples = `r ncol(cs3_norm) - 3`
  * Genes = `r nrow(cs3_norm)`

The number of genes are reduced by 19: 5 housekeeping, 8 negative, 6 positive (the latter 2 types are not used).

### Common Samples and Genes

Since the reference pool samples only exist in CS2 and CS3, we need to find an alternative method to normalize all three CodeSets. One method is to select common samples and common genes that exist in all three. We found `r length(common_genes)` common genes. Using the `summaryID` identifier, we also found `r length(common_id)` common summary IDs, translating to `r length(common_samples)` samples. The number of samples that were matched to each CodeSet differed:

* CS1: OvCa2103_C953
  * Samples = `r nrow(cs1_clean)`
  * Genes = `r ncol(cs1_clean) - 2`
* CS2: PrOTYPE2_v2_C1645
  * Samples = `r nrow(cs2_clean)`
  * Genes = `r ncol(cs2_clean) - 2`
* CS3: OTTA2014_C2822
  * Samples = `r nrow(cs3_clean)`
  * Genes = `r ncol(cs3_clean) - 2`

#### Overlap of common samples by summary ID

```{r venn-id}
grid.draw(venn_id)
```

#### Overlap of common genes

```{r venn-genes}
grid.draw(venn_genes)
```

*Excluding housekeeping genes and controls
  
### CS1 Training Set Generation

We use the reference method to normalize CS1 to CS3.

* CS1 reference set: duplicate samples from CS1
  * Samples = `r nrow(cs1_ref2)`
  * Genes = `r ncol(cs1_ref2)`
* CS3 reference set: corresponding samples in CS3 also found in CS1 reference set
  * Samples = `r nrow(cs3_ref)`
  * Genes = `r ncol(cs3_ref)`
* CS1 validation set: remaining CS1 samples with reference set removed
  * Samples = `r nrow(cs1_val2)`
  * Genes = `r ncol(cs1_val2)`

The final CS1 training set has `r nrow(cs1_data)` samples on `r ncol(cs1_data)` genes after normalization and keeping only the major histotypes of interest.

### CS2 Training Set Generation

We use the pool method to normalize CS2 to CS3 so we can be consistent with the PrOType normalization when there are available pools.

* CS2 pools:
  * Samples = 12 (Pool 1 = 4, Pool 2 = 4, Pool 3 = 4)
  * Genes = `r nrow(cs2_pools)`
* CS3 pools:
  * Samples = `r ncol(ref_pools)` (Pool 1 = 12, Pool 2 = 5, Pool 3 = 5)
  * Genes = `r nrow(ref_pools)`
* CS2 validation set: CS2 samples with pools removed
  * Samples = `r ncol(cs2_norm) - 3 - 9`
  * Genes = `r nrow(cs2_norm)`

The final CS2 training set has `r nrow(cs2_data)` samples on `r ncol(cs2_data)` (common) genes after normalization and keeping only the major histotypes of interest.

### Cohort Distribution

Note that the CS3 pools sample total (n=58) shown here include those that are not used as reference pools, following previous normalization methods. In particular, the distribution of CS3 pools actually used for normalization (n=22) is POOL1 = 12, POOL2 = 5, POOL3 = 5.

```{r cohort-dist}
source(here::here("validation/cs_process_cohorts.R"))
cohort_dist <- cohorts %>% 
  filter(col_name %in% c(cs1_samples, cs2_samples, cs3_samples)) %>% 
  count(file_source, cohort) %>% 
  pivot_wider(names_from = "file_source", values_from = "n")

knitr::kable(cohort_dist,
             caption = "Cohort Distribution amongst CodeSets")
```

## Classification

We use 6 classification algorithms and 4 subsampling methods across 500 repetitions in the supervised learning framework for CS1 and CS2. The pipeline was run using many SGE batch jobs as a way of parallelization on a CentOS 5 server. Implementations of the techniques below were called from the [splendid](https://alinetalhouk.github.io/splendid/) package. 

- Classifiers:
  - Random Forest
  - Adaboost
  - XGBoost
  - LDA
  - SVM
  - K-Nearest Neighbours
- Subsampling:
  - None
  - Down-sampling
  - Up-sampling
  - SMOTE
