# Methods

```{r setup-02}
library(recipes)
library(themis)
library(ggplot2)
library(dplyr)
library(rlang)
library(here)
train_data <- readRDS(here("data/train_data.rds"))
train_class <- readRDS(here("data/train_class.rds"))
```

## Normalization

The full training set was comprised of data from CodeSet (CS) 1, 2, and 3. All CodeSets were first normalized to housekeeping genes, then a different approach was taken for each of the CodeSets.

CS1 was normalized to CS3 using "Random1" reference samples. These reference samples are common samples between CS1 and CS3, randomly selected such that we obtain one from each of the five histotypes. Then we use the reference method to normalize CS1 to CS3.

Similarly, CS2 was normalized to CS3 using "Random1" reference samples using five common samples between CS2 and CS3 such that there is one from each histotype.

For CS3, we first split the dataset by site: Vancouver, USC, and AOC. We use the CS3-Vancouver subset as a "reference standard", so we normalized CS3-USC and CS3-AOC to CS3-Vancouver using a "Random1" reference method where we reference samples are common between USC and Vancouver, and between AOC and Vancouver. The CS3-Vancouver is also included without further normalization.

## Case Selection

Duplicate cases (two samples with the same ottaID) were removed from the training set before fitting the classification models. CS3 cases were preferred over CS1 and CS2, and CS3-Vancouver were preferred over CS3-AOC and CS3-USC.

The training, confirmation, and validation sets all used a different set of cohorts.

## Classification

We use 5 classification algorithms and 4 subsampling methods across 500 repetitions in the supervised learning framework for the Training Set, CS1 and CS2. The pipeline was run using SLURM batch jobs submitted to a partition on a CentOS 7 server. Implementations of the techniques below were called from the [splendid](https://alinetalhouk.github.io/splendid/) package.

-   Classifiers:

    -   Random Forest
    -   Support Vector Machine
    -   XGBoost
    -   Regularized Multinomial Regression Model

-   Subsampling:

    -   None
    -   Down-sampling
    -   Up-sampling
    -   SMOTE
    
## Subsampling

The figure below helps visualize how the distribution of classes changes when we apply subsampling techniques to handle class imbalance:

```{r sampling, fig.cap='Visualization of Subsampling Techniques'}
train_ref <- cbind(train_data, class = train_class)
rec <- recipe(class ~ ., train_ref)

preproc_none <- rec
preproc_down <- step_downsample(rec, class, seed = 2024)
preproc_up <- step_upsample(rec, class, seed = 2024)
preproc_smote <- step_smote(rec, class, seed = 2024)
preproc <- list(
    none = preproc_none,
    down = preproc_down,
    up = preproc_up,
    smote = preproc_smote
)

sampling_df <- map(preproc, ~ {
  exec(prep, .x) %>%
    pluck("template") %>%
    count(class)
}) %>%
  list_rbind(names_to = "Sampling") %>%
  mutate(Sampling = factor(Sampling, levels = c("none", "down", "up", "smote")),
         class = fct_reorder(class, n, .desc = TRUE))

p_sampling <-
  ggplot(sampling_df, aes(x = Sampling, y = n, fill = class)) +
  geom_col(position = "dodge") +
  scale_fill_brewer(palette = "Dark2") +
  labs(y = "Count", title = "Visualization of Subsampling Techniques") +
  theme_bw()

print(p_sampling)
```
